{"meta":{"title":"Hexo","subtitle":"","description":"","author":"John Doe","url":"http://example.com","root":"/"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2023-06-04T13:14:15.187Z","updated":"2023-06-04T13:14:15.187Z","comments":false,"path":"/404.html","permalink":"http://example.com/404.html","excerpt":"","text":""},{"title":"书单","date":"2023-06-04T13:14:15.192Z","updated":"2023-06-04T13:14:15.192Z","comments":false,"path":"books/index.html","permalink":"http://example.com/books/index.html","excerpt":"","text":""},{"title":"关于","date":"2023-06-04T13:14:15.191Z","updated":"2023-06-04T13:14:15.191Z","comments":false,"path":"about/index.html","permalink":"http://example.com/about/index.html","excerpt":"","text":"个人详细介绍"},{"title":"分类","date":"2023-06-04T13:14:15.192Z","updated":"2023-06-04T13:14:15.192Z","comments":false,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2023-06-04T13:14:15.192Z","updated":"2023-06-04T13:14:15.192Z","comments":true,"path":"links/index.html","permalink":"http://example.com/links/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2023-06-04T13:14:15.193Z","updated":"2023-06-04T13:14:15.193Z","comments":false,"path":"repository/index.html","permalink":"http://example.com/repository/index.html","excerpt":"","text":""},{"title":"标签","date":"2023-06-04T13:14:15.193Z","updated":"2023-06-04T13:14:15.193Z","comments":false,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"记一下常用的pg sql","slug":"SQL小记(pg in 安图)","date":"2021-08-30T02:30:20.000Z","updated":"2023-06-04T13:14:15.190Z","comments":true,"path":"2021/08/30/SQL小记(pg in 安图)/","link":"","permalink":"http://example.com/2021/08/30/SQL%E5%B0%8F%E8%AE%B0(pg%20in%20%E5%AE%89%E5%9B%BE)/","excerpt":"","text":"SQL小记geomotry类型问题123456789--新增 、赋值、删除 、修改名称alter table &quot;tmp-505b2dbe-005b-4e03-846a-17ca21798032&quot; add geometry2 geometry(MULTIPOLYGON,4496);UPDATE &quot;tmp-505b2dbe-005b-4e03-846a-17ca21798032&quot; SET geometry2 = geometry;alter table &quot;tmp-505b2dbe-005b-4e03-846a-17ca21798032&quot; DROP COLUMN geometry;alter table &quot;tmp-505b2dbe-005b-4e03-846a-17ca21798032&quot; RENAME geometry2 to geometry--更改坐标系 SELECT UpdateGeometrySRID(&#x27;config_overlay_tmp_table_4496&#x27;, &#x27;geometry&#x27;, 4496); pg元数据12--查询空间字段信息SELECT * FROM public.geometry_columns pg创建空间表123drop TABLE test_config_overlay_tmp_table_4496;CREATE TABLE test_config_overlay_tmp_table_4496 (ID VARCHAR NOT NULL PRIMARY KEY, task_id VARCHAR NOT NULL , geometry geometry(MULTIPOLYGON,4496) ); 清除空闲连接1234--查询连接SELECT * FROM pg_stat_activity ;--清除空闲SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE &quot;state&quot; = &#x27;idle&#x27;; 更新空间字段类型123456789101112--将geometry 换为geometry2alter table overlay_tmp.tmp_common_smart_1613981987802 add geometry2 geometry(MULTIPOLYGON, 900914) ;UPDATE overlay_tmp.tmp_common_smart_1613981987802 SET geometry = geometry2; alter table overlay_tmp.tmp_common_smart_1613981987802 drop column geometry; alter table overlay_tmp.tmp_common_smart_1613981987802 RENAME COLUMN geometry2 to geometry;--直接更新--更新字段类型UPDATE overlay_tmp.tmp_common_smart_1613981987803 SET geometry = st_geomfromewkt(&#x27;SRID=900914;&#x27;||replace(st_astext(geometry), &#x27;POLYGON&#x27;, &#x27;MULTIPOLYGON(&#x27;) || &#x27;)&#x27;) WHERE st_geometrytype(geometry) = &#x27;ST_Polygon&#x27;; alter table overlay_tmp.tmp_common_smart_1613981987803 ALTER column geometry TYPE geometry(MULTIPOLYGON, 900914); 投影表123select * from information_schema.foreign_tables;--查询外部表drop FOREIGN TABLE tmp_common_smart_1614586290758; --删除import FOREIGN SCHEMA overlay_tmp limit to ( &quot;tmp_common_smart_1614586290758&quot; ) from server server_remote_yztcddb INTO public; --投影 查询表主键12345678910111213SELECT pg_constraint.conname AS pk_name, pg_attribute.attname AS colname, pg_type.typname AS typename FROM pg_constraint INNER JOIN pg_class ON pg_constraint.conrelid = pg_class.oid INNER JOIN pg_attribute ON pg_attribute.attrelid = pg_class.oid AND pg_attribute.attnum = pg_constraint.conkey [ 1 ] INNER JOIN pg_type ON pg_type.oid = pg_attribute.atttypid WHERE pg_class.relname = &#x27;config_overlay_tmp_table_4496&#x27; AND pg_constraint.contype = &#x27;p&#x27;; 修改字段名称1ALTER TABLE &quot;$TABLENAME&quot; RENAME $SOURCECOL TO $TARGETCOL 查询字段是否存在1select count(*) from information_schema.columns WHERE table_schema = &#x27;public&#x27; and table_name = &#x27;$TABLENAME&#x27; and column_name = &#x27;$COLNAME&#x27;; 新增序列1CREATE SEQUENCE $SEQNAME INCREMENT 1 START 1 MINVALUE 1 MAXVALUE 9223372036854775807 CACHE 1 创建字段并指定序列1ALTER TABLE &quot;$TABLENAME&quot; ADD COLUMN smid integer NOT NULL DEFAULT nextval(&#x27;$SEQNAME&#x27;::regclass) 删除主键1ALTER TABLE &quot;$TABLENAME&quot; DROP CONSTRAINT $PKNAME 指定字段为主键1ALTER TABLE &quot;$TABLENAME&quot; ADD CONSTRAINT $TABLENAME_pkey PRIMARY KEY (smid) 处理ST_GEOMETRYCOLLECTION参考：https://postgis.net/docs/ST_CollectionExtract.html 123456789101112-- Constants: 1 == POINT, 2 == LINESTRING, 3 == POLYGONSELECT ST_AsText(ST_CollectionExtract(ST_GeomFromText(&#x27;GEOMETRYCOLLECTION(GEOMETRYCOLLECTION(POINT(0 0)))&#x27;),1));st_astext---------------MULTIPOINT(0 0)(1 row)SELECT ST_AsText(ST_CollectionExtract(ST_GeomFromText(&#x27;GEOMETRYCOLLECTION(GEOMETRYCOLLECTION(LINESTRING(0 0, 1 1)),LINESTRING(2 2, 3 3))&#x27;),2));st_astext---------------MULTILINESTRING((0 0, 1 1), (2 2, 3 3))(1 row) 查询表以及表注释1234select relname as tablename,cast(obj_description(relfilenode,&#x27;pg_class&#x27;) as varchar) as comment from pg_class where relname = &#x27;tmp_difference_tqdbtb&#x27; ; 三维转二维 st_force2d() 正向递归 123456789101112131415161718192021-- with recursive cte as( -- 先查询root节点 select id, dir_name, parent_id, &#x27;&#x27;::VARCHAR as parent_dir_name, dir_name::TEXT as branch from gbp_datastore.datastore_dir where id = &#x27;6ec14bae768ab40bc5d0a2d8561e8a5a&#x27; union all -- 通过cte递归查询root节点的直接子节点 select origin.id, origin.dir_name, cte.id as parent_id, cte.dir_name as parent_dir_name, cte.branch || &#x27;/&#x27; || origin.dir_name from cte join gbp_datastore.datastore_dir as origin on origin.parent_id = cte.id)select id,dir_name, parent_id, parent_dir_name, branch, -- 通过计算分隔符的个数，模拟计算出树形的深度 (length(branch)-length(replace(branch, &#x27;/&#x27;, &#x27;&#x27;))) as lvlfrom cte;","categories":[],"tags":[{"name":"gis","slug":"gis","permalink":"http://example.com/tags/gis/"},{"name":"postgis","slug":"postgis","permalink":"http://example.com/tags/postgis/"},{"name":"sql","slug":"sql","permalink":"http://example.com/tags/sql/"}]},{"title":"Spring SchedulingConfigurer 实现动态定时任务","slug":"Spring SchedulingConfigurer 实现动态定时任务","date":"2021-08-30T02:30:20.000Z","updated":"2023-06-04T13:14:15.190Z","comments":true,"path":"2021/08/30/Spring SchedulingConfigurer 实现动态定时任务/","link":"","permalink":"http://example.com/2021/08/30/Spring%20SchedulingConfigurer%20%E5%AE%9E%E7%8E%B0%E5%8A%A8%E6%80%81%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/","excerpt":"","text":"Spring SchedulingConfigurer 实现动态定时任务一、前言大家在日常工作中，一定使用过 Spring 的 @Scheduled 注解吧，通过该注解可以非常方便的帮助我们实现任务的定时执行。 但是该注解是不支持运行时动态修改执行间隔的，不知道你在业务中有没有这些需求和痛点： 在服务运行时能够动态修改定时任务的执行频率和执行开关，而无需重启服务和修改代码 能够基于配置，在不同环境/机器上，实现定时任务执行频率的差异化 这些都可以通过 Spring 的 SchedulingConfigurer 注解来实现。 这个注解其实大家并不陌生，如果有使用过 @Scheduled 的话，因为 @Scheduled 默认是单线程执行的，因此如果存在多个任务同时触发，可能触发阻塞。使用 SchedulingConfigurer 可以配置用于执行 @Scheduled 的线程池，来避免这个问题。 12345678@Configurationpublic class ScheduleConfig implements SchedulingConfigurer &#123; @Override public void configureTasks(ScheduledTaskRegistrar taskRegistrar) &#123; //设定一个长度10的定时任务线程池 taskRegistrar.setScheduler(Executors.newScheduledThreadPool(10)); &#125;&#125; 但其实这个接口，还可以实现动态定时任务的功能，下面来演示如何实现。 二、功能实现 后续定义的类开头的 DS 是 Dynamic Schedule 的缩写。 使用到的依赖，除了 Spring 外，还包括： 1234567891011121314151617&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-collections4&lt;/artifactId&gt; &lt;version&gt;4.4&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;version&gt;1.18.18&lt;/version&gt;&lt;/dependency&gt; 2.1 @EnableScheduling首先需要开启 @EnableScheduling 注解，直接在启动类添加即可： 1234567@EnableScheduling@SpringBootApplicationpublic class DSApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(DSApplication.class, args); &#125;&#125; 2.2 IDSTaskInfo定义一个任务信息的接口，后续所有用于动态调整的任务信息对象，都需要实现该接口。 id：该任务信息的唯一 ID，用于唯一标识一个任务 cron：该任务执行的 cron 表达式。 isValid：任务开关 isChange：用于标识任务参数是否发生了改变 123456789101112131415161718192021public interface IDSTaskInfo &#123; /** * 任务 ID */ long getId(); /** * 任务执行 cron 表达式 */ String getCron(); /** * 任务是否有效 */ boolean isValid(); /** * 判断任务是否发生变化 */ boolean isChange(IDSTaskInfo oldTaskInfo);&#125; 2.3 DSContainer顾名思义，是存放 IDSTaskInfo 的容器。 具有以下成员变量： ```scheduleMap 123456789101112131415 ：用于暂存 IDSTaskInfo 和实际任务 ScheduledTask 的映射关系。其中： - &#96;task_id&#96;：作为主键，确保一个 IDSTaskInfo 只会被注册进一次 - &#96;T&#96;：暂存当初注册时的 IDSTaskInfo，用于跟最新的 IDSTaskInfo 比较参数是否发生变化 - &#96;ScheduledTask&#96;：暂存当初注册时生成的任务，如果需要取消任务的话，需要拿到该对象 - &#96;Semaphore&#96;：确保每个任务实际执行时只有一个线程执行，不会产生并发问题 -&#96;taskRegistrar&#96;：Spring 的任务注册管理器，用于注册任务到 Spring 容器中- &#96;name&#96;：调用方提供的类名具有以下成员方法：- &#96;&#96;&#96;java void checkTask(final T taskInfo, final TriggerTask triggerTask) ：检查 IDSTaskInfo，判断是否需要注册/取消任务。具体的逻辑包括： 如果任务已经注册： 如果任务无效：则取消任务 如果任务有效： 如果任务配置发生了变化：则取消任务并重新注册任务 如果任务没有注册： 如果任务有效：则注册任务 Semaphore getSemaphore()：获取信号量属性。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081import lombok.extern.slf4j.Slf4j;import org.apache.commons.lang3.tuple.Pair;import org.springframework.scheduling.config.ScheduledTask;import org.springframework.scheduling.config.ScheduledTaskRegistrar;import org.springframework.scheduling.config.TriggerTask;import java.util.Map;import java.util.concurrent.ConcurrentHashMap;import java.util.concurrent.Semaphore;/** * 存放 IDSTaskInfo 容器 * @author jitwxs * @date 2021年03月27日 16:29 */@Slf4jpublic class DSContainer&lt;T extends IDSTaskInfo&gt; &#123; /** * IDSTaskInfo和真实任务的关联关系 * * &lt;task_id, &lt;Task, &lt;Scheduled, Semaphore&gt;&gt;&gt; */ private final Map&lt;Long, Pair&lt;T, Pair&lt;ScheduledTask, Semaphore&gt;&gt;&gt; scheduleMap = new ConcurrentHashMap&lt;&gt;(); private final ScheduledTaskRegistrar taskRegistrar; private final String name; public DSContainer(ScheduledTaskRegistrar scheduledTaskRegistrar, final String name) &#123; this.taskRegistrar = scheduledTaskRegistrar; this.name = name; &#125; /** * 注册任务 * @param taskInfo 任务信息 * @param triggerTask 任务的触发规则 */ public void checkTask(final T taskInfo, final TriggerTask triggerTask) &#123; final long taskId = taskInfo.getId(); if (scheduleMap.containsKey(taskId)) &#123; if (taskInfo.isValid()) &#123; final T oldTaskInfo = scheduleMap.get(taskId).getLeft(); if(oldTaskInfo.isChange(taskInfo)) &#123; log.info(&quot;DSContainer will register &#123;&#125; again because task config change, taskId: &#123;&#125;&quot;, name, taskId); cancelTask(taskId); registerTask(taskInfo, triggerTask); &#125; &#125; else &#123; log.info(&quot;DSContainer will cancelTask &#123;&#125; because task not valid, taskId: &#123;&#125;&quot;, name, taskId); cancelTask(taskId); &#125; &#125; else &#123; if (taskInfo.isValid()) &#123; log.info(&quot;DSContainer will register &#123;&#125; task, taskId: &#123;&#125;&quot;, name, taskId); registerTask(taskInfo, triggerTask); &#125; &#125; &#125; /** * 获取 Semaphore，确保任务不会被多个线程同时执行 */ public Semaphore getSemaphore(final long taskId) &#123; return this.scheduleMap.get(taskId).getRight().getRight(); &#125; private void registerTask(final T taskInfo, final TriggerTask triggerTask) &#123; final ScheduledTask latestTask = taskRegistrar.scheduleTriggerTask(triggerTask); this.scheduleMap.put(taskInfo.getId(), Pair.of(taskInfo, Pair.of(latestTask, new Semaphore(1)))); &#125; private void cancelTask(final long taskId) &#123; final Pair&lt;T, Pair&lt;ScheduledTask, Semaphore&gt;&gt; pair = this.scheduleMap.remove(taskId); if (pair != null) &#123; pair.getRight().getLeft().cancel(); &#125; &#125;&#125; 2.4 AbstractDSHandler下面定义实际的动态线程池处理方法，这里采用抽象类实现，将共用逻辑封装起来，方便扩展。 具有以下抽象方法： List&lt;T&gt; listTaskInfo()：获取所有的任务信息。 void doProcess(T taskInfo)：实现实际执行任务的业务逻辑。 具有以下公共方法： void configureTasks(ScheduledTaskRegistrar taskRegistrar)：创建 DSContainer 对象，并创建一个单线程的任务定时执行，调用 scheduleTask() 方法处理实际逻辑。 void scheduleTask()：首先加载所有任务信息，然后基于 cron 表达式生成 TriggerTask 对象，调用 checkTask() 方法确认是否需要注册/取消任务。当达到执行时间时，调用 execute() 方法，执行任务逻辑。 void execute(final T taskInfo)：获取信号量，成功后执行任务逻辑。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081import lombok.extern.slf4j.Slf4j;import org.apache.commons.collections4.CollectionUtils;import org.springframework.scheduling.annotation.SchedulingConfigurer;import org.springframework.scheduling.config.ScheduledTaskRegistrar;import org.springframework.scheduling.config.TriggerTask;import org.springframework.scheduling.support.CronTrigger;import java.util.List;import java.util.Objects;import java.util.concurrent.Semaphore;import java.util.concurrent.TimeUnit;/** * 抽象 Dynamic Schedule 实现，基于 SchedulingConfigurer 实现 * @author jitwxs * @date 2021年03月27日 16:41 */@Slf4jpublic abstract class AbstractDSHandler&lt;T extends IDSTaskInfo&gt; implements SchedulingConfigurer &#123; private DSContainer&lt;T&gt; dsContainer; private final String CLASS_NAME = getClass().getSimpleName(); /** * 获取所有的任务信息 */ protected abstract List&lt;T&gt; listTaskInfo(); /** * 做具体的任务逻辑 * * &lt;p/&gt; 该方法执行时位于跟 SpringBoot @Scheduled 注解相同的线程池内。如果内部仍需要开子线程池执行，请务必同步等待子线程池执行完毕，否则可能会影响预期效果。 */ protected abstract void doProcess(T taskInfo) throws Throwable; @Override public void configureTasks(ScheduledTaskRegistrar taskRegistrar) &#123; dsContainer = new DSContainer&lt;&gt;(taskRegistrar, CLASS_NAME); // 每隔 100ms 调度一次，用于读取所有任务 taskRegistrar.addFixedDelayTask(this::scheduleTask, 1000); &#125; /** * 调度任务，加载所有任务并注册 */ private void scheduleTask() &#123; CollectionUtils.emptyIfNull(listTaskInfo()).forEach(taskInfo -&gt; dsContainer.checkTask(taskInfo, new TriggerTask(() -&gt; this.execute(taskInfo), triggerContext -&gt; new CronTrigger(taskInfo.getCron()).nextExecutionTime(triggerContext) )) ); &#125; private void execute(final T taskInfo) &#123; final long taskId = taskInfo.getId(); try &#123; Semaphore semaphore = dsContainer.getSemaphore(taskId); if (Objects.isNull(semaphore)) &#123; log.error(&quot;&#123;&#125; semaphore is null, taskId: &#123;&#125;&quot;, CLASS_NAME, taskId); return; &#125; if (semaphore.tryAcquire(3, TimeUnit.SECONDS)) &#123; try &#123; doProcess(taskInfo); &#125; catch (Throwable throwable) &#123; log.error(&quot;&#123;&#125; doProcess error, taskId: &#123;&#125;&quot;, CLASS_NAME, taskId, throwable); &#125; finally &#123; semaphore.release(); &#125; &#125; else &#123; log.warn(&quot;&#123;&#125; too many executor, taskId: &#123;&#125;&quot;, CLASS_NAME, taskId); &#125; &#125; catch (InterruptedException e) &#123; log.warn(&quot;&#123;&#125; interruptedException error, taskId: &#123;&#125;&quot;, CLASS_NAME, taskId); &#125; catch (Exception e) &#123; log.error(&quot;&#123;&#125; execute error, taskId: &#123;&#125;&quot;, CLASS_NAME, taskId, e); &#125; &#125;&#125; 三、快速测试至此就完成了动态任务的框架搭建，下面让我们来快速测试下。为了尽量减少其他技术带来的复杂度，本次测试不涉及数据库和真实的定时任务，完全采用模拟实现。 3.1 模拟定时任务为了模拟一个定时任务，我定义了一个 foo() 方法，其中只输出一句话。后续我将通过定时调用该方法，来模拟定时任务。 12345678910import lombok.extern.slf4j.Slf4j;import java.time.LocalTime;@Slf4jpublic class SchedulerTest &#123; public void foo() &#123; log.info(&quot;&#123;&#125; Execute com.github.jitwxs.sample.ds.test.SchedulerTest#foo&quot;, LocalTime.now()); &#125;&#125; 3.2 实现 IDSTaskInfo首先定义 IDSTaskInfo，我这里想通过反射来实现调用 foo() 方法，因此 reference 表示的是要调用方法的全路径。另外我实现了 isChange() 方法，只要 cron、isValid、reference 发生了变动，就认为该任务的配置发生了改变。 12345678910111213141516171819202122232425import com.github.jitwxs.sample.ds.config.IDSTaskInfo;import lombok.Builder;import lombok.Data;@Data@Builderpublic class SchedulerTestTaskInfo implements IDSTaskInfo &#123; private long id; private String cron; private boolean isValid; private String reference; @Override public boolean isChange(IDSTaskInfo oldTaskInfo) &#123; if(oldTaskInfo instanceof SchedulerTestTaskInfo) &#123; final SchedulerTestTaskInfo obj = (SchedulerTestTaskInfo) oldTaskInfo; return !this.cron.equals(obj.cron) || this.isValid != obj.isValid || !this.reference.equals(obj.getReference()); &#125; else &#123; throw new IllegalArgumentException(&quot;Not Support SchedulerTestTaskInfo type&quot;); &#125; &#125;&#125; 3.3 实现 AbstractDSHandler有几个需要关注的： listTaskInfo() 返回值我使用了 volatile 变量，便于我修改它，模拟任务信息数据的改变。 doProcess() 方法中，读取到 reference 后，使用反射进行调用，模拟定时任务的执行。 额外实现了 1ApplicationListener 接口，当服务启动后，每隔一段时间修改下任务信息，模拟业务中调整配置。 服务启动后，foo() 定时任务将每 10s 执行一次。 10s 后，将 foo() 定时任务执行周期从每 10s 执行调整为 1s 执行。 10s 后，关闭 foo() 定时任务执行。 10s 后，开启 foo() 定时任务执行。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990import com.github.jitwxs.sample.ds.config.AbstractDSHandler;import org.springframework.context.ApplicationEvent;import org.springframework.context.ApplicationListener;import org.springframework.stereotype.Component;import java.lang.reflect.Method;import java.util.Collections;import java.util.List;import java.util.concurrent.Executors;import java.util.concurrent.TimeUnit;import java.util.concurrent.locks.LockSupport;/** * @author jitwxs * @date 2021年03月27日 21:54 */@Componentpublic class SchedulerTestDSHandler extends AbstractDSHandler&lt;SchedulerTestTaskInfo&gt; implements ApplicationListener &#123; public volatile List&lt;SchedulerTestTaskInfo&gt; taskInfoList = Collections.singletonList( SchedulerTestTaskInfo.builder() .id(1) .cron(&quot;0/10 * * * * ? &quot;) .isValid(true) .reference(&quot;com.github.jitwxs.sample.ds.test.SchedulerTest#foo&quot;) .build() ); @Override protected List&lt;SchedulerTestTaskInfo&gt; listTaskInfo() &#123; return taskInfoList; &#125; @Override protected void doProcess(SchedulerTestTaskInfo taskInfo) throws Throwable &#123; final String reference = taskInfo.getReference(); final String[] split = reference.split(&quot;#&quot;); if(split.length != 2) &#123; return; &#125; try &#123; final Class&lt;?&gt; clazz = Class.forName(split[0]); final Method method = clazz.getMethod(split[1]); method.invoke(clazz.newInstance()); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; @Override public void onApplicationEvent(ApplicationEvent applicationEvent) &#123; Executors.newScheduledThreadPool(1).scheduleAtFixedRate(() -&gt; &#123; LockSupport.parkNanos(TimeUnit.SECONDS.toNanos(10)); // setting 1 seconds execute taskInfoList = Collections.singletonList( SchedulerTestTaskInfo.builder() .id(1) .cron(&quot;0/1 * * * * ? &quot;) .isValid(true) .reference(&quot;com.github.jitwxs.sample.ds.test.SchedulerTest#foo&quot;) .build() ); LockSupport.parkNanos(TimeUnit.SECONDS.toNanos(10)); // setting not valid taskInfoList = Collections.singletonList( SchedulerTestTaskInfo.builder() .id(1) .cron(&quot;0/1 * * * * ? &quot;) .isValid(false) .reference(&quot;com.github.jitwxs.sample.ds.test.SchedulerTest#foo&quot;) .build() ); LockSupport.parkNanos(TimeUnit.SECONDS.toNanos(10)); // setting valid taskInfoList = Collections.singletonList( SchedulerTestTaskInfo.builder() .id(1) .cron(&quot;0/1 * * * * ? &quot;) .isValid(true) .reference(&quot;com.github.jitwxs.sample.ds.test.SchedulerTest#foo&quot;) .build() ); &#125;, 12, 86400, TimeUnit.SECONDS); &#125;&#125; 3.4 运行程序整个应用包结构如下： 运行程序后，在控制台可以观测到如下输出： 四、后记 以上完成了动态定时任务的介绍，你能够根据本篇文章，实现以下需求吗： 本文基于 cron 表达式实现了频率控制，你能改用 fixedDelay 或 fixedRate 实现吗？ 基于数据库/配置文件/配置中心，实现对服务中定时任务的动态频率调整和任务的启停。 开发一个数据表历史数据清理功能，能够动态配置要清理的表、清理的规则、清理的周期。 开发一个数据表异常数据告警功能，能够动态配置要扫描的表、告警的规则、扫描的周期。","categories":[],"tags":[{"name":"spring","slug":"spring","permalink":"http://example.com/tags/spring/"},{"name":"schedule","slug":"schedule","permalink":"http://example.com/tags/schedule/"}]},{"title":"K8s 1.17 证书更新","slug":"K8s 1.17 证书更新","date":"2021-05-27T13:12:30.000Z","updated":"2023-06-04T13:14:15.188Z","comments":true,"path":"2021/05/27/K8s 1.17 证书更新/","link":"","permalink":"http://example.com/2021/05/27/K8s%201.17%20%E8%AF%81%E4%B9%A6%E6%9B%B4%E6%96%B0/","excerpt":"","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# #检查证书有效期[root@k8s-master ~]# kubeadm alpha certs check-expirationCERTIFICATE EXPIRES RESIDUAL TIME EXTERNALLY MANAGEDadmin.conf Sep 25, 2020 11:32 UTC 184d noapiserver Sep 25, 2020 11:32 UTC 184d noapiserver-etcd-client Sep 25, 2020 11:20 UTC 184d noapiserver-kubelet-client Sep 25, 2020 11:32 UTC 184d nocontroller-manager.conf Sep 25, 2020 11:32 UTC 184d noetcd-healthcheck-client Sep 25, 2020 11:20 UTC 184d noetcd-peer Sep 25, 2020 11:20 UTC 184d noetcd-server Sep 25, 2020 11:19 UTC 184d nofront-proxy-client Sep 25, 2020 11:32 UTC 184d noscheduler.conf Sep 25, 2020 11:32 UTC 184d no# # 在一台 master 上执行更新证书的操作[root@k8s-master ~]# kubeadm alpha certs renew allcertificate embedded in the kubeconfig file for the admin to use and for kubeadm itself renewedcertificate for serving the Kubernetes API renewedcertificate the apiserver uses to access etcd renewedcertificate for the API server to connect to kubelet renewedcertificate embedded in the kubeconfig file for the controller manager to use renewedcertificate for liveness probes to healthcheck etcd renewedcertificate for etcd nodes to communicate with each other renewedcertificate for serving etcd renewedcertificate for the front proxy client renewedcertificate embedded in the kubeconfig file for the scheduler manager to use renewed# # 注：如果少了，运行 kubectl 命令时会报错 error: You must be logged in to the server (Unauthorized)cp -i /etc/kubernetes/admin.conf $HOME/.kube/config # #再次检查证书有效期[root@k8s-master ~]# kubeadm alpha certs check-expirationCERTIFICATE EXPIRES RESIDUAL TIME EXTERNALLY MANAGEDadmin.conf Mar 25, 2021 03:07 UTC 364d noapiserver Mar 25, 2021 03:07 UTC 364d noapiserver-etcd-client Mar 25, 2021 03:07 UTC 364d noapiserver-kubelet-client Mar 25, 2021 03:07 UTC 364d nocontroller-manager.conf Mar 25, 2021 03:07 UTC 364d noetcd-healthcheck-client Mar 25, 2021 03:07 UTC 364d noetcd-peer Mar 25, 2021 03:07 UTC 364d noetcd-server Mar 25, 2021 03:07 UTC 364d nofront-proxy-client Mar 25, 2021 03:07 UTC 364d noscheduler.conf Mar 25, 2021 03:07 UTC 364d no# #重启监控程序[root@k8s-master ~]# docker ps | grep -v pause | grep -E &quot;etcd|scheduler|controller|apiserver&quot; | awk &#x27;&#123;print $1&#125;&#x27; | awk &#x27;&#123;print &quot;docker&quot;,&quot;restart&quot;,$1&#125;&#x27; | bash","categories":[],"tags":[{"name":"kubernets","slug":"kubernets","permalink":"http://example.com/tags/kubernets/"}]},{"title":"CopyOnWriteArrayList源码解析","slug":"CopyOnWriteArrayList源码解析","date":"2021-05-26T13:12:30.000Z","updated":"2023-06-04T13:14:15.188Z","comments":true,"path":"2021/05/26/CopyOnWriteArrayList源码解析/","link":"","permalink":"http://example.com/2021/05/26/CopyOnWriteArrayList%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/","excerpt":"","text":"CopyOnWriteArrayList为线程安全的ArrayList，这节分析下CopyOnWriteArrayList的源码，基于JDK1.8。 类结构CopyOnWriteArrayList类关系图： CopyOnWriteArrayList实现了List接口的所有方法，主要包含如下两个成员变量： 12345// 可重入锁，用于对写操作加锁final transient ReentrantLock lock = new ReentrantLock();// Object类型数组，存放数据，volatile修饰，目的是一个线程对这个字段的修改另外一个线程立即可见private transient volatile Object[] array; CopyOnWriteArrayList中并没有和容量有关的属性或者常量，下面通过对一些常用方法的源码解析，就可以知道原因。 方法解析构造函数CopyOnWriteArrayList()空参构造函数： 1234567public CopyOnWriteArrayList() &#123; setArray(new Object[0]);&#125;final void setArray(Object[] a) &#123; array = a;&#125; 无参构造函数直接创建了一个长度为0的Object数组。 CopyOnWriteArrayList(Collection&lt;? extends E&gt; c)： 123456789101112131415public CopyOnWriteArrayList(Collection&lt;? extends E&gt; c) &#123; Object[] elements; if (c.getClass() == CopyOnWriteArrayList.class) // 如果集合类型就是CopyOnWriteArrayList，则直接将其array赋值给当前CopyOnWriteArrayList elements = ((CopyOnWriteArrayList&lt;?&gt;)c).getArray(); else &#123; // 如果不是CopyOnWriteArrayList类型，则将集合转换为数组 elements = c.toArray(); // 就如ArrayList源码分析所述那样，c.toArray()返回类型不一定是Object[].class，所以需要转换 if (elements.getClass() != Object[].class) elements = Arrays.copyOf(elements, elements.length, Object[].class); &#125; // 设置array值 setArray(elements);&#125; CopyOnWriteArrayList(E[] toCopyIn)： 1234public CopyOnWriteArrayList(E[] toCopyIn) &#123; // 入参为数组，拷贝一份赋值给array setArray(Arrays.copyOf(toCopyIn, toCopyIn.length, Object[].class));&#125; add(E e)add(E e)往CopyOnWriteArrayList末尾添加元素： 1234567891011121314151617181920212223242526public boolean add(E e) &#123; // 获取可重入锁 final ReentrantLock lock = this.lock; // 上锁，同一时间内只能有一个线程进入 lock.lock(); try &#123; // 获取当前array属性值 Object[] elements = getArray(); // 获取当前array数组长度 int len = elements.length; // 复制一份新数组，新数组长度为当前array数组长度+1 Object[] newElements = Arrays.copyOf(elements, len + 1); // 在新数组末尾添加元素 newElements[len] = e; // 新数组赋值给array属性 setArray(newElements); return true; &#125; finally &#123; // 锁释放 lock.unlock(); &#125;&#125;final Object[] getArray() &#123; return array;&#125; 可以看到，add操作通过ReentrantLock来确保线程安全。通过add方法，我们也可以看出CopyOnWriteArrayList修改操作的基本思想为：复制一份新的数组，新数组长度刚好能够容纳下需要添加的元素；在新数组里进行操作；最后将新数组赋值给array属性，替换旧数组。这种思想也称为“写时复制”，所以称为CopyOnWriteArrayList。 此外，我们可以看到CopyOnWriteArrayList中并没有类似于ArrayList的grow方法扩容的操作。 add(int index, E element)add(int index, E element)指定下标添加指定元素： 123456789101112131415161718192021222324252627282930313233343536public void add(int index, E element) &#123; // 获取可重入锁 final ReentrantLock lock = this.lock; // 上锁，同一时间内只能有一个线程进入 lock.lock(); try &#123; // 获取当前array属性值 Object[] elements = getArray(); // 获取当前array数组长度 int len = elements.length; // 下标检查 if (index &gt; len || index &lt; 0) throw new IndexOutOfBoundsException(&quot;Index: &quot;+index+ &quot;, Size: &quot;+len); Object[] newElements; int numMoved = len - index; if (numMoved == 0) // numMoved为0，说明是在末尾添加，过程和add(E e)方法一致 newElements = Arrays.copyOf(elements, len + 1); else &#123; // 否则创建一个新数组，数组长度为旧数组长度值+1 newElements = new Object[len + 1]; // 分两次复制，分别将index之前和index+1之后的元素复制到新数组中 System.arraycopy(elements, 0, newElements, 0, index); System.arraycopy(elements, index, newElements, index + 1, numMoved); &#125; // 在新数组的index位置添加指定元素 newElements[index] = element; // 新数组赋值给array属性，替换旧数组 setArray(newElements); &#125; finally &#123; // 锁释放 lock.unlock(); &#125;&#125; set(int index, E element)set(int index, E element)设置指定位置的值： 123456789101112131415161718192021222324252627282930313233public E set(int index, E element) &#123; // 获取可重入锁 final ReentrantLock lock = this.lock; // 上锁，同一时间内只能有一个线程进入 lock.lock(); try &#123; // 获取当前array属性值 Object[] elements = getArray(); // 获取当前array指定index下标值 E oldValue = get(elements, index); if (oldValue != element) &#123; // 如果新值和旧值不相等 int len = elements.length; // 复制一份新数组，长度和旧数组一致 Object[] newElements = Arrays.copyOf(elements, len); // 修改新数组index下标值 newElements[index] = element; // 新数组赋值给array属性，替换旧数组 setArray(newElements); &#125; else &#123; // 即使新值和旧值一致，为了确保volatile语义，需要重新设置array setArray(elements); &#125; return oldValue; &#125; finally &#123; // 释放锁 lock.unlock(); &#125;&#125;private E get(Object[] a, int index) &#123; return (E) a[index];&#125; remove(int index)remove(int index)删除指定下标元素： 1234567891011121314151617181920212223242526272829303132public E remove(int index) &#123; // 获取可重入锁 final ReentrantLock lock = this.lock; // 上锁，同一时间内只能有一个线程进入 try &#123; // 获取当前array属性值 Object[] elements = getArray(); // 获取当前array长度 int len = elements.length; // 获取旧值 E oldValue = get(elements, index); int numMoved = len - index - 1; if (numMoved == 0) // 如果删除的是最后一个元素，则将当前array设置为新数组 // 新数组长度为旧数组长度-1，这样刚好截去了最后一个元素 setArray(Arrays.copyOf(elements, len - 1)); else &#123; // 分段复制，将index前的元素和index+1后的元素复制到新数组 // 新数组长度为旧数组长度-1 Object[] newElements = new Object[len - 1]; System.arraycopy(elements, 0, newElements, 0, index); System.arraycopy(elements, index + 1, newElements, index, numMoved); // 设置array setArray(newElements); &#125; return oldValue; &#125; finally &#123; // 锁释放 lock.unlock(); &#125;&#125; 可以看到，CopyOnWriteArrayList中的增删改操作都是在新数组中进行的，并且通过加锁的方式确保同一时刻只能有一个线程进行操作，操作完后赋值给array属性，替换旧数组，旧数组失去了引用，最终由GC回收。 get(int index)123456public E get(int index) &#123; return get(getArray(), index);&#125;final Object[] getArray() &#123; return array;&#125; 可以看到，get(int index)操作是分两步进行的： 通过getArray()获取array属性值； 获取array数组index下标值。 这个过程并没有加锁，所以在并发环境下可能出现如下情况： 线程1调用get(int index)方法获取值，内部通过getArray()方法获取到了array属性值； 线程2调用CopyOnWriteArrayList的增删改方法，内部通过setArray方法修改了array属性的值； 线程1还是从旧的array数组中取值。 所以get方法是弱一致性的。 size()123public int size() &#123; return getArray().length;&#125; size()方法返回当前array属性长度，因为CopyOnWriteArrayList中的array数组每次复制都刚好能够容纳下所有元素，并不像ArrayList那样会预留一定的空间。所以CopyOnWriteArrayList中并没有size属性，元素的个数和数组的长度是相等的。 迭代器12345678910111213141516171819public Iterator&lt;E&gt; iterator() &#123; return new COWIterator&lt;E&gt;(getArray(), 0);&#125;static final class COWIterator&lt;E&gt; implements ListIterator&lt;E&gt; &#123; /** Snapshot of the array */ private final Object[] snapshot; /** Index of element to be returned by subsequent call to next. */ private int cursor; private COWIterator(Object[] elements, int initialCursor) &#123; cursor = initialCursor; snapshot = elements; &#125; public boolean hasNext() &#123; return cursor &lt; snapshot.length; &#125; ......&#125; 可以看到，迭代器也是弱一致性的，并没有在锁中进行。如果其他线程没有对CopyOnWriteArrayList进行增删改的操作，那么snapshot还是创建迭代器时获取的array，但是如果其他线程对CopyOnWriteArrayList进行了增删改的操作，旧的数组会被新的数组给替换掉，但是snapshot还是原来旧的数组的引用： 1234567CopyOnWriteArrayList&lt;String&gt; list = new CopyOnWriteArrayList&lt;&gt;();list.add(&quot;hello&quot;);Iterator&lt;String&gt; iterator = list.iterator();list.add(&quot;world&quot;);while (iterator.hasNext())&#123; System.out.println(iterator.next());&#125; 输出结果仅为hello。 总结 CopyOnWriteArrayList体现了写时复制的思想，增删改操作都是在复制的新数组中进行的； CopyOnWriteArrayList的取值方法是弱一致性的，无法确保实时取到最新的数据； CopyOnWriteArrayList的增删改方法通过可重入锁确保线程安全； CopyOnWriteArrayList线程安全体现在多线程增删改不会抛出java.util.ConcurrentModificationException异常，并不能确保数据的强一致性； 同一时刻只能有一个线程对CopyOnWriteArrayList进行增删改操作，而读操作没有限制，并且 CopyOnWriteArrayList增删改操作都需要复制一份新数组，增加了内存消耗，所以CopyOnWriteArrayList适合读多写少的情况。","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"}]},{"title":"CopyOnWriteArraySet源码解析","slug":"CopyOnWriteArraySet源码解析","date":"2021-05-26T13:12:30.000Z","updated":"2023-06-04T13:14:15.188Z","comments":true,"path":"2021/05/26/CopyOnWriteArraySet源码解析/","link":"","permalink":"http://example.com/2021/05/26/CopyOnWriteArraySet%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/","excerpt":"","text":"CopyOnWriteArraySet为线程安全的Set实现，本文记录CopyOnWriteArraySet源码解析，基于JDK1.8。 类结构先来看下CopyOnWriteArraySet的类层级关系图： 没什么好说的。再来看看内部属性： 12// 就一个属性，CopyOnWriteArraySet内部采用CopyOnWriteArrayList存储元素private final CopyOnWriteArrayList&lt;E&gt; al; 和HashSet不一样的是，CopyOnWriteArraySet内部采用CopyOnWriteArrayList存储元素，这也是CopyOnWriteArraySet名字的由来，因为CopyOnWriteArrayList是线程安全的，CopyOnWriteArraySet的方法都是基于CopyOnWriteArrayList实现的，所以CopyOnWriteArraySet自然而然也是线程安全的，同样的，在并发环境下获取数据是弱一致性的！ 方法解析构造函数123456789101112131415161718192021// 空参构造函数，实际就是初始化CopyOnWriteArrayListpublic CopyOnWriteArraySet() &#123; al = new CopyOnWriteArrayList&lt;E&gt;();&#125;// 传入集合对象public CopyOnWriteArraySet(Collection&lt;? extends E&gt; c) &#123; // 分两种情况 if (c.getClass() == CopyOnWriteArraySet.class) &#123; // 如果集合就是CopyOnWriteArraySet类型，说明数据是不重复的 // 直接全部添加到CopyOnWriteArrayList中 @SuppressWarnings(&quot;unchecked&quot;) CopyOnWriteArraySet&lt;E&gt; cc = (CopyOnWriteArraySet&lt;E&gt;)c; al = new CopyOnWriteArrayList&lt;E&gt;(cc.al); &#125; else &#123; // 否则调用addAllAbsent添加所有当前集合中不存在的元素，确保数据的唯一性 al = new CopyOnWriteArrayList&lt;E&gt;(); al.addAllAbsent(c); &#125;&#125; add(E e)add(E e)添加指定元素： 12345public boolean add(E e) &#123; // 实际调用CopyOnWriteArrayList的addIfAbsent方法 // 元素不存在，则添加，返回true；元素存在，则不添加，返回false return al.addIfAbsent(e);&#125; 可以看到，CopyOnWriteArraySet的add方法通过调用CopyOnWriteArrayList的addIfAbsent来确保元素不重复，以满足Set的特性。 剩下方法略剩下方法都比较简单，都是直接调用CopyOnWriteArrayList方法实现，感兴趣自己阅读源码。","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"}]},{"title":"HashSet源码解析","slug":"HashSet源码解析","date":"2021-05-26T13:12:30.000Z","updated":"2023-06-04T13:14:15.188Z","comments":true,"path":"2021/05/26/HashSet源码解析/","link":"","permalink":"http://example.com/2021/05/26/HashSet%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/","excerpt":"","text":"本文记录HashSet源码解析，基于JDK1.8。 类结构HashSet类层级关系图： HashSet实现了Set接口，为什么叫HashSet？因为HashSet内部采用哈希表（实际就是HashMap）来存储不重复的数据，查看HashSet内部属性： 1234// 使用HashMap存储数据，HashSet的数据实际为HashMap的keyprivate transient HashMap&lt;E,Object&gt; map;// HashMap value占位符private static final Object PRESENT = new Object(); HashMap的key是不允许重复的，这也正好符合Set的特性。因为HashSet内部采用HashMap存储数据，所以HashSet可以存储null值，支持快速失败，非线程安全。 map属性通过transient修饰，原因在介绍HashMap源码的时候分析过。 方法解析构造函数1234567891011121314151617181920212223// 空参构造函数，内部初始化map属性public HashSet() &#123; map = new HashMap&lt;&gt;();&#125;// 传入集合对象public HashSet(Collection&lt;? extends E&gt; c) &#123; // 初始化map，计算map的容量 // 计算公式为 c.size/0.75f + 1，如果值小于16，则取值16。 map = new HashMap&lt;&gt;(Math.max((int) (c.size()/.75f) + 1, 16)); // 将集合中的所有元素添加进去 addAll(c);&#125;// 手动指定容量和加载因子public HashSet(int initialCapacity, float loadFactor) &#123; map = new HashMap&lt;&gt;(initialCapacity, loadFactor);&#125;// 手动指定容量public HashSet(int initialCapacity) &#123; map = new HashMap&lt;&gt;(initialCapacity);&#125; 可以看到，创建HashSet的本质就是初始化HashMap。 add(E e)add(E e)添加指定元素： 1234public boolean add(E e) &#123; // 往map里添加元素，如果key已经存在则返回false，否则返回true return map.put(e, PRESENT)==null;&#125; contains(Object o)contains(Object o)判断是否包含指定元素： 1234public boolean contains(Object o) &#123; // 本质就是判断map中是否包含该key return map.containsKey(o);&#125; size()size()获取元素个数： 1234public int size() &#123; // 本质就是获取map的元素个数 return map.size();&#125; isEmpty()isEmpty()判断集合是否为空： 1234public boolean isEmpty() &#123; // 本质就是判断map是否为空 return map.isEmpty();&#125; remove(Object o)remove(Object o)删除指定元素： 1234public boolean remove(Object o) &#123; // 本质就是通过key删除map中的元素，如果该key存在，则返回true，否则返回false return map.remove(o)==PRESENT;&#125; clear()clear()清空集合： 1234public void clear() &#123; // 本质就是清空map map.clear();&#125; iterator()iterator()获取迭代器： 1234public Iterator&lt;E&gt; iterator() &#123; // 本质就是获取map key的迭代器 return map.keySet().iterator();&#125;","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"}]},{"title":"LinkedHashSet源码解析","slug":"LinkedHashSet源码解析","date":"2021-05-26T13:12:30.000Z","updated":"2023-06-04T13:14:15.189Z","comments":true,"path":"2021/05/26/LinkedHashSet源码解析/","link":"","permalink":"http://example.com/2021/05/26/LinkedHashSet%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/","excerpt":"","text":"我们知道，HashSet内部使用HashMap存储元素，所以HashSet遍历数据时是无序的，要保证插入的元素有序，我们可以使用LinkedHashSet。本节记录LinkedHashSet源码解析，基于JDK1.8。 LinkedHashSet类层级关系如下所示： LinkedHashSet继承自HashSet。查看LinkedHashSet的构造方法源码会发现内部都是调用父类的HashSet(int initialCapacity, float loadFactor, boolean dummy)方法： 123HashSet(int initialCapacity, float loadFactor, boolean dummy) &#123; map = new LinkedHashMap&lt;&gt;(initialCapacity, loadFactor);&#125; dummy参数没有任何意义，仅用于和别的入参为(int,float)的构造器区分开。方法内部创建的是LinkedHashMap，所以LinkedHashSet就是用LinkedHashMap来保证插入元素有序的，对LinkedHashMap不熟悉的请参考LinkedHashMap源码解析。 从上面的代码我们还可以发现，LinkedHashSet无法改变linkedHashMap的accessOrder属性值，所以在LinkedHashSet中，元素的顺序只能和插入顺序一致： 123456LinkedHashSet&lt;String&gt; set = new LinkedHashSet&lt;&gt;();set.add(&quot;apple&quot;);set.add(&quot;orange&quot;);set.add(&quot;watermelon&quot;);set.add(&quot;strawberry&quot;);System.out.println(set); 输出顺序和插入顺序一致： 1[apple, orange, watermelon, strawberry]","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"}]},{"title":"Vector源码解析","slug":"Vector源码解析","date":"2021-05-26T13:12:30.000Z","updated":"2023-06-04T13:14:15.190Z","comments":true,"path":"2021/05/26/Vector源码解析/","link":"","permalink":"http://example.com/2021/05/26/Vector%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/","excerpt":"","text":"Vector和ArrayList非常相似，它们都实现了相同的接口，继承相同的类，就连方法的实现也非常类似。和ArrayList不同的是，Vector是线程安全的，关键方法上都加了synchronized同步锁，由于Vector效率不高，所以使用的较少，要使用线程安全的ArrayList，推荐CopyOnWriteArrayList，后续再做分析，这里仅记录下Vector源码，基于JDK1.8。 类结构Vector的类关系图和ArrayList一致： Vector可以存放任意类型元素（包括null），允许重复，和ArrayList一致，内部采用Object类型数组存放数据，包含以下三个成员变量： 12345678// Object数组，存放数据protected Object[] elementData;// 元素个数protected int elementCount;// 当数组容量不足时，容量增加capacityIncrement，如果capacityIncrement为0，则扩容为2倍protected int capacityIncrement; 方法解析构造函数12345678910111213141516public Vector(int initialCapacity, int capacityIncrement) &#123; super(); if (initialCapacity &lt; 0) throw new IllegalArgumentException(&quot;Illegal Capacity: &quot;+ initialCapacity); this.elementData = new Object[initialCapacity]; this.capacityIncrement = capacityIncrement;&#125;public Vector(int initialCapacity) &#123; this(initialCapacity, 0);&#125;public Vector() &#123; this(10);&#125; 可以看到，当我们调用new Vector()创建Vector集合时，直接创建了一个容量为10的Object数组（和ArrayList不同，ArrayList内部数组初始容量为0，只有在添加第一个元素的时候才扩容为10），并且capacityIncrement为0，意味着容量不足时，新数组容量为旧数组容量的2倍。 add(E e)123456789101112131415161718192021222324252627282930313233public synchronized boolean add(E e) &#123; modCount++; ensureCapacityHelper(elementCount + 1); elementData[elementCount++] = e; return true;&#125;private void ensureCapacityHelper(int minCapacity) &#123; // overflow-conscious code if (minCapacity - elementData.length &gt; 0) grow(minCapacity);&#125;private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; // capacityIncrement为0的话，新容量为旧容量的2倍，不为0，则新容量为旧容量+capacityIncrement int newCapacity = oldCapacity + ((capacityIncrement &gt; 0) ? capacityIncrement : oldCapacity); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); elementData = Arrays.copyOf(elementData, newCapacity);&#125;private static int hugeCapacity(int minCapacity) &#123; if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); return (minCapacity &gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE;&#125; 添加逻辑和ArrayList的add方法大体一致，区别在于扩容策略有些不同，并且方法使用synchronized关键字修饰。 set(int index, E element)12345678public synchronized E set(int index, E element) &#123; if (index &gt;= elementCount) throw new ArrayIndexOutOfBoundsException(index); E oldValue = elementData(index); elementData[index] = element; return oldValue;&#125; 逻辑和ArrayList的set方法一致，方法使用synchronized关键字修饰。 get(int index)123456789public synchronized E get(int index) &#123; if (index &gt;= elementCount) throw new ArrayIndexOutOfBoundsException(index); return elementData(index);&#125;E elementData(int index) &#123; return (E) elementData[index];&#125; 逻辑和ArrayList的get方法一致，方法使用synchronized关键字修饰。 remove(int index)1234567891011121314public synchronized E remove(int index) &#123; modCount++; if (index &gt;= elementCount) throw new ArrayIndexOutOfBoundsException(index); E oldValue = elementData(index); int numMoved = elementCount - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--elementCount] = null; // Let gc do its work return oldValue;&#125; 逻辑和ArrayList的remove方法一致，方法使用synchronized关键字修饰。 trimToSize()1234567public synchronized void trimToSize() &#123; modCount++; int oldCapacity = elementData.length; if (elementCount &lt; oldCapacity) &#123; elementData = Arrays.copyOf(elementData, elementCount); &#125;&#125; 逻辑和ArrayList的trimToSize方法一致，方法使用synchronized关键字修饰。 剩下的方法源码自己查看，大体和ArrayList没有什么区别。Vector的方法都用synchronized关键字来确保线程安全，每次只有一个线程能访问此对象，在线程竞争激烈的情况下，这种方法效率非常低，所以实际并不推荐使用Vector。","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"}]},{"title":"postgrep安装配置","slug":"postgrep","date":"2021-05-26T13:12:30.000Z","updated":"2023-06-04T13:14:15.191Z","comments":true,"path":"2021/05/26/postgrep/","link":"","permalink":"http://example.com/2021/05/26/postgrep/","excerpt":"","text":"1、postgrep创建超级用户打开安装目录的PostgreSQL\\10\\scripts -&gt; runpsql.bat 1postgres=# CREATE ROLE admin superuser PASSWORD &#x27;admin&#x27; login; 2、给数据库添加空间扩展创建普通数据库 1CREATE DATABASE dbname; 给数据库添加空间扩展,切换刚才创建的数据库下，运行下面的命令即可： 12345CREATE EXTENSION postgis;CREATE EXTENSION postgis_topology;CREATE EXTENSION fuzzystrmatch;CREATE EXTENSION postgis_tiger_geocoder;CREATE EXTENSION address_standardizer;","categories":[],"tags":[{"name":"Postgrep","slug":"Postgrep","permalink":"http://example.com/tags/Postgrep/"}]},{"title":"分布式锁","slug":"分布式锁","date":"2021-05-26T13:12:30.000Z","updated":"2023-06-04T13:14:15.191Z","comments":true,"path":"2021/05/26/分布式锁/","link":"","permalink":"http://example.com/2021/05/26/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/","excerpt":"","text":"Java 提供了两种内置的锁的实现，一种是由 JVM 实现的 synchronized 和 JDK 提供的 Lock，对于单机单进程应用，可以使用它们来实现锁。当应用涉及到多机、多进程共同完成时，那么这时候就需要一个全局锁来实现多个进程之间的同步。 使用场景在服务器端使用分布式部署的情况下，一个服务可能分布在不同的节点上，比如订单服务分布式在节点 A 和节点 B 上。如果多个客户端同时对一个服务进行请求时，就需要使用分布式锁。例如一个服务可以使用 APP 端或者 Web 端进行访问，如果一个用户同时使用 APP 端和 Web 端访问该服务，并且 APP 端的请求路由到了节点 A，WEB 端的请求被路由到了节点 B，这时候就需要使用分布式锁来进行同步。 实现方式1. 数据库分布式锁（一）基于 MySQL 锁表 该实现完全依靠数据库的唯一索引。当想要获得锁时，就向数据库中插入一条记录，释放锁时就删除这条记录。如果记录具有唯一索引，就不会同时插入同一条记录。 这种方式存在以下几个问题： 锁没有失效时间，解锁失败会导致死锁，其他线程无法再获得锁。 只能是非阻塞锁，插入失败直接就报错了，无法重试。 不可重入，同一线程在没有释放锁之前无法再获得锁。 （二）采用乐观锁增加版本号 根据版本号来判断更新之前有没有其他线程更新过，如果被更新过，则获取锁失败。 2. Redis 分布式锁（一）基于 SETNX、EXPIRE 使用 SETNX（set if not exist）命令插入一个键值对时，如果 Key 已经存在，那么会返回 False，否则插入成功并返回 True。因此客户端在尝试获得锁时，先使用 SETNX 向 Redis 中插入一个记录，如果返回 True 表示获得锁，返回 False 表示已经有客户端占用锁。 EXPIRE 可以为一个键值对设置一个过期时间，从而避免了死锁的发生。 （二）RedLock 算法 RedLock 算法使用了多个 Redis 实例来实现分布式锁，这是为了保证在发生单点故障时还可用。 尝试从 N 个相互独立 Redis 实例获取锁，如果一个实例不可用，应该尽快尝试下一个。 计算获取锁消耗的时间，只有当这个时间小于锁的过期时间，并且从大多数（N/2+1）实例上获取了锁，那么就认为锁获取成功了。 如果锁获取失败，会到每个实例上释放锁。 3. Zookeeper 分布式锁Zookeeper 是一个为分布式应用提供一致性服务的软件，例如配置管理、分布式协同以及命名的中心化等，这些都是分布式系统中非常底层而且是必不可少的基本功能，但是如果自己实现这些功能而且要达到高吞吐、低延迟同时还要保持一致性和可用性，实际上非常困难。 （一）抽象模型 Zookeeper 提供了一种树形结构级的命名空间，/app1/p_1 节点表示它的父节点为 /app1。 （二）节点类型 永久节点：不会因为会话结束或者超时而消失； 临时节点：如果会话结束或者超时就会消失； 有序节点：会在节点名的后面加一个数字后缀，并且是有序的，例如生成的有序节点为 /lock/node-0000000000，它的下一个有序节点则为 /lock/node-0000000001，依次类推。 （三）监听器 为一个节点注册监听器，在节点状态发生改变时，会给客户端发送消息。 （四）分布式锁实现 创建一个锁目录 /lock。 在 /lock 下创建临时的且有序的子节点，第一个客户端对应的子节点为 /lock/lock-0000000000，第二个为 /lock/lock-0000000001，以此类推。 客户端获取 /lock 下的子节点列表，判断自己创建的子节点是否为当前子节点列表中序号最小的子节点，如果是则认为获得锁，否则监听自己的前一个子节点，获得子节点的变更通知后重复此步骤直至获得锁； 执行业务代码，完成后，删除对应的子节点。 （五）会话超时 如果一个已经获得锁的会话超时了，因为创建的是临时节点，因此该会话对应的临时节点会被删除，其它会话就可以获得锁了。可以看到，Zookeeper 分布式锁不会出现数据库分布式锁的死锁问题。 （六）羊群效应 在步骤二，一个节点未获得锁，需要监听监听自己的前一个子节点，这是因为如果监听所有的子节点，那么任意一个子节点状态改变，其它所有子节点都会收到通知（羊群效应），而我们只希望它的后一个子节点收到通知。","categories":[],"tags":[{"name":"分布式","slug":"分布式","permalink":"http://example.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}]},{"title":"ArrayList & LinkedList源码解析","slug":"ArrayList & LinkedList源码解析","date":"2021-05-26T13:12:30.000Z","updated":"2023-06-04T13:14:15.187Z","comments":true,"path":"2021/05/26/ArrayList & LinkedList源码解析/","link":"","permalink":"http://example.com/2021/05/26/ArrayList%20&%20LinkedList%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/","excerpt":"","text":"本文记录ArrayList &amp; LinkedList源码解析，基于JDK1.8。 ArrayListArrayList实现了List接口的所有方法，可以看成是“长度可调节的数组”，可以包含任何类型数据（包括null，可重复）。ArrayList大体和Vector一致，唯一区别是ArrayList非线程安全，Vector线程安全，但Vector线程安全的代价较大，推荐使用CopyOnWriteArrayList，后面文章再做记录。 类结构ArrayList类层级关系如下图所示： ArrayList额外实现了RandomAccess接口，关于RandomAccess接口的作用下面再做讨论。 ArrayList类主要包含如下两个成员变量： 12345678public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable&#123; transient Object[] elementData; private int size; ......&#125; elementData为Object类型数组，用于存放ArrayList数据；size表示数组元素个数（并非数组容量）。 ArrayList类还包含了一些常量： 12345678910public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable&#123; // 数组默认初始化容量为10 private static final int DEFAULT_CAPACITY = 10; // 表示空数组 private static final Object[] EMPTY_ELEMENTDATA = &#123;&#125;; // 也是空数组，和EMPTY_ELEMENTDATA区分开 private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = &#123;&#125;;&#125; 方法解析知识储备Arrays类的copyOf(U[] original, int newLength, Class&lt;? extends T[]&gt; newType)方法用于复制指定数组original到新数组，新数组的长度为newLength，新数组元素类型为newType。 如果新数组的长度大于旧数组，那么多出的那部分用null填充； 如果新数组的长度小于旧数组，那么少的那部分直接截取掉。 举两个例子： 123456Long[] array1 = new Long[]&#123;1L, 2L, 3L&#125;;Object[] array2 = Arrays.copyOf(array1, 5, Object[].class);System.out.println(Arrays.toString(array2)); // [1, 2, 3, null, null]Object[] array3 = Arrays.copyOf(array1, 1, Object[].class);System.out.println(Arrays.toString(array3)); // [1] 重载方法copyOf(T[] original, int newLength)用于复制指定数组original到新数组，新数组的长度为newLength，新数组元素类型和旧数组一致。 copyOf方法内部调用System类的native方法arraycopy(Object src, int srcPos,Object dest, int destPos, int length)： src：需要被拷贝的旧数组； srcPos：旧数组开始拷贝的起始位置； dest：拷贝目标数组； destPos：目标数组的起始拷贝位置； length：拷贝的长度。 举例： 1234Long[] array1 = new Long[]&#123;1L, 2L, 3L&#125;;Object[] array2 = new Object[5];System.arraycopy(array1, 0, array2, 0, 3);System.out.println(Arrays.toString(array2)); // [1, 2, 3, null, null] 指定位置插入元素： 12345Long[] array1 = new Long[]&#123;1L, 2L, 3L, null, null, null&#125;;int index = 1;System.arraycopy(array1, index, array1, index + 1, 3 - index);array1[index] = 0L;System.out.println(Arrays.toString(array1)); // [1, 0, 2, 3, null, null] 构造函数public ArrayList(int initialCapacity)： 12345678910public ArrayList(int initialCapacity) &#123; if (initialCapacity &gt; 0) &#123; this.elementData = new Object[initialCapacity]; &#125; else if (initialCapacity == 0) &#123; this.elementData = EMPTY_ELEMENTDATA; &#125; else &#123; throw new IllegalArgumentException(&quot;Illegal Capacity: &quot;+ initialCapacity); &#125;&#125; 创建容量大小为initialCapacity的ArrayList，如果initialCapacity小于0，则抛出IllegalArgumentException异常；如果initialCapacity为0，则elementData为EMPTY_ELEMENTDATA。 public ArrayList()： 123public ArrayList() &#123; this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA;&#125; 空参构造函数，elementData为DEFAULTCAPACITY_EMPTY_ELEMENTDATA。 public ArrayList(Collection&lt;? extends E&gt; c)： 1234567891011public ArrayList(Collection&lt;? extends E&gt; c) &#123; elementData = c.toArray(); if ((size = elementData.length) != 0) &#123; // c.toArray might (incorrectly) not return Object[] (see 6260652) if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size, Object[].class); &#125; else &#123; // replace with empty array. this.elementData = EMPTY_ELEMENTDATA; &#125;&#125; 创建一个包含指定集合c数据的ArrayList。上面为什么要多此一举使用Arrays.copyOf(elementData, size, Object[].class)复制一遍数组呢？这是因为在某些情况下调用集合的toArray()方法返回的类型并不是Object[].class，比如： 1234567Long[] array1 = &#123;1L, 2L&#125;;List&lt;Long&gt; list1 = Arrays.asList(array1);Object[] array2 = list1.toArray();System.out.println(array2.getClass() == Object[].class); // falseList&lt;Long&gt; list2 = new ArrayList&lt;&gt;();System.out.println(list2.toArray().getClass() == Object[].class); // true add(E e)add(E e)用于尾部添加元素： 123456public boolean add(E e) &#123; // 用于确定数组容量 ensureCapacityInternal(size + 1); elementData[size++] = e; return true;&#125; 假如现在我们通过如下代码创建了一个ArrayList实例： 12ArrayList&lt;String&gt; list = new ArrayList&lt;&gt;();list.add(&quot;hello&quot;); 内部过程如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public boolean add(E e) &#123; // 用于确定数组容量，e=hello，size=0 ensureCapacityInternal(size + 1); // 末尾添加元素，然后size递增1 elementData[size++] = e; return true;&#125;private void ensureCapacityInternal(int minCapacity) &#123; // minCapacity=1,elementData=&#123;&#125; ensureExplicitCapacity(calculateCapacity(elementData, minCapacity));&#125;private static int calculateCapacity(Object[] elementData, int minCapacity) &#123; if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; // DEFAULT_CAPACITY=10，minCapacity=1，故返回10 return Math.max(DEFAULT_CAPACITY, minCapacity); &#125; return minCapacity;&#125;private void ensureExplicitCapacity(int minCapacity) &#123; // minCapacity=10 modCount++; // minCapacity=10，elementData.length=0，所以调用grow方法扩容 if (minCapacity - elementData.length &gt; 0) grow(minCapacity);&#125;private void grow(int minCapacity) &#123; //minCapacity=10 // oldCapacity=0 int oldCapacity = elementData.length; // newCapacity为oldCapacity的1.5倍，这里为0 int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); // newCapacity=0，minCapacity=10，所以该条件成立 if (newCapacity - minCapacity &lt; 0) // newCapacity=10 newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // 复制到新数组，数组容量为10 elementData = Arrays.copyOf(elementData, newCapacity);&#125;private static int hugeCapacity(int minCapacity) &#123; if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); // MAX_ARRAY_SIZE常量值为Integer.MAX_VALUE - 8，通过 // 这段逻辑我们可以知道，ArrayList最大容量为Integer.MAX_VALUE return (minCapacity &gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE;&#125; 通过上面源码分析我们可以知道： 任何一个空的ArrayList在添加第一个元素时，内部数组容量将被扩容为10； 扩容时，newCapacity为oldCapacity的1.5倍； 数组容量最大为Integer.MAX_VALUE； 尾部添加元素不用移动任何元素，所以速度快。 add(int index, E element)add(int index, E element)用于在指定位置添加元素： 12345678910111213141516171819public void add(int index, E element) &#123; // 下标检查 rangeCheckForAdd(index); // 确定数组容量，和上面add(E e)方法介绍的一致 ensureCapacityInternal(size + 1); // 将原来index后面的所有元素往后面移动一个位置 System.arraycopy(elementData, index, elementData, index + 1, size - index); // index处放入新元素 elementData[index] = element; // size递增 size++;&#125;private void rangeCheckForAdd(int index) &#123; // 下标比size大或者下标小于0，都会抛出下标越界异常 if (index &gt; size || index &lt; 0) throw new IndexOutOfBoundsException(outOfBoundsMsg(index));&#125; 这里涉及到元素移动，所以速度较慢。 get(int index)get(int index)获取指定位置元素： 12345678910public E get(int index) &#123; // 下标合法性检查 rangeCheck(index); // 直接返回数组指定位置元素 return elementData(index);&#125;E elementData(int index) &#123; return (E) elementData[index];&#125; get方法直接返回数组指定下标元素，速度非常快。 set(int index, E element)set(int index, E element)设置指定位置元素为指定值： 12345678910public E set(int index, E element) &#123; // 下标合法性检查 rangeCheck(index); // 根据下标获取旧值 E oldValue = elementData(index); // 设置新值 elementData[index] = element; // 返回旧值 return oldValue;&#125; set方法不涉及元素移动和遍历，所以速度快。 remove(int index)remove(int index)删除指定位置元素： 12345678910111213141516public E remove(int index) &#123; rangeCheck(index); modCount++; // 获取指定位置元素（需要被删除的元素） E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved &gt; 0) // 直接将index后面的元素往前移动一位，覆盖index处的元素 System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work // 返回被删除的元 return oldValue;&#125; 上述方法涉及到元素移动，所以效率也不高。 remove(Object o)remove(Object o)删除指定元素： 1234567891011121314151617181920212223242526// 遍历数组，找到第一个目标元素，然后删除public boolean remove(Object o) &#123; if (o == null) &#123; for (int index = 0; index &lt; size; index++) if (elementData[index] == null) &#123; fastRemove(index); return true; &#125; &#125; else &#123; for (int index = 0; index &lt; size; index++) if (o.equals(elementData[index])) &#123; fastRemove(index); return true; &#125; &#125; return false;&#125;// 逻辑和remove一致，都是将index后面的元素往前移动一位，覆盖index处的元素private void fastRemove(int index) &#123; modCount++; int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work&#125; 方法涉及到数组遍历和元素移动，效率也不高。 trimToSize()trimToSize()源码： 12345678public void trimToSize() &#123; modCount++; if (size &lt; elementData.length) &#123; elementData = (size == 0) ? EMPTY_ELEMENTDATA : Arrays.copyOf(elementData, size); &#125;&#125; 该方法用于将数组容量调整为实际元素个数大小，当一个ArrayList元素个数不会发生改变时，可以调用该方法减少内存占用。 其他方法可以自己阅读ArrayList源码，此外在涉及增删改的方法里，我们都看到了modCount++操作，和之前介绍HashMap源码时一致，用于快速失败。 LinkedList类结构LinkedList底层采用双向链表结构存储数据，允许重复数据和null值，长度没有限制： 每个节点用内部类Node表示： 1234567891011private static class Node&lt;E&gt; &#123; E item; Node&lt;E&gt; next; Node&lt;E&gt; prev; Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) &#123; this.item = element; this.next = next; this.prev = prev; &#125;&#125; Node节点包含item（存储数据），next（后继节点）和prev（前继节点）。数组内存地址必须连续，而链表就没有这个限制了，Node可以分布于各个内存地址，它们之间的关系通过prev和next维护。 LinkedList类关系图： 可以看到LinkedList类并没有实现RandomAccess接口，额外实现了Deque接口，所以包含一些队列方法。 LinkedList包含如下成员变量： 12345678// 元素个数，默认为0transient int size = 0;// 表示第一个节点，第一个节点必须满足(first == null &amp;&amp; last == null) || (first.prev == null &amp;&amp; first.item != null)transient Node&lt;E&gt; first;// 表示最后一个节点，最后一个节点必须满足(first == null &amp;&amp; last == null) || (last.next == null &amp;&amp; last.item != null)transient Node&lt;E&gt; last; 方法解析构造函数LinkedList()： 12public LinkedList() &#123;&#125; 空参构造函数，默认size为0，每次添加新元素都要创建Node节点。 LinkedList(Collection&lt;? extends E&gt; c)： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public LinkedList(Collection&lt;? extends E&gt; c) &#123; this(); addAll(c);&#125;public boolean addAll(Collection&lt;? extends E&gt; c) &#123; return addAll(size, c);&#125;public boolean addAll(int index, Collection&lt;? extends E&gt; c) &#123; checkPositionIndex(index); Object[] a = c.toArray(); int numNew = a.length; if (numNew == 0) return false; Node&lt;E&gt; pred, succ; if (index == size) &#123; succ = null; pred = last; &#125; else &#123; succ = node(index); pred = succ.prev; &#125; // 循环创建节点，设置prev，next指向 for (Object o : a) &#123; @SuppressWarnings(&quot;unchecked&quot;) E e = (E) o; Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, null); if (pred == null) first = newNode; else pred.next = newNode; pred = newNode; &#125; if (succ == null) &#123; last = pred; &#125; else &#123; pred.next = succ; succ.prev = pred; &#125; size += numNew; modCount++; return true;&#125; 该构造函数用于创建LinkedList，并往里添加指定集合元素。 add(int index, E element)add(int index, E element)指定下标插入元素： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475public void add(int index, E element) &#123; // 下标合法性检查 checkPositionIndex(index); if (index == size) // 如果插入下标等于size，说明是在尾部插入，执行尾部插入操作 linkLast(element); else // 如果不是尾插入，则在指定下标节点前插入 linkBefore(element, node(index));&#125;private void checkPositionIndex(int index) &#123; if (!isPositionIndex(index)) throw new IndexOutOfBoundsException(outOfBoundsMsg(index));&#125;private boolean isPositionIndex(int index) &#123; return index &gt;= 0 &amp;&amp; index &lt;= size;&#125;void linkLast(E e) &#123; // 获取最后一个节点 final Node&lt;E&gt; l = last; // 创建一个新节点，prev为原链表最后一个节点，next为null final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null); // 更新last为新节点 last = newNode; if (l == null) // 如果原链表最后一个节点为null，说明原链表没有节点，将新节点赋给first first = newNode; else // 否则更新原链表最后一个节点的next为新节点 l.next = newNode; // size递增 size++; // 模数递增，用于快速失败 modCount++;&#125;void linkBefore(E e, Node&lt;E&gt; succ) &#123; // succ为原链表指定index位置的节点，获取其prev节点 final Node&lt;E&gt; pred = succ.prev; // 创建新节点，prev为原链表指定index位置的节点的prev节点，next为原链表指定index位置的节点 final Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, succ); // 将原链表指定index位置的节点的prev更新为新节点 succ.prev = newNode; if (pred == null) // 如果链表指定index位置的节点的prev为null，说明原链表没有节点，将新节点赋给first first = newNode; else // 否则更新原链表指定index位置的节点的prev的next节点为新节点 pred.next = newNode; // size递增 size++; // 模数递增，用于快速失败 modCount++;&#125;// 采用二分法遍历每个Node节点，直到找到index位置的节点Node&lt;E&gt; node(int index) &#123; // assert isElementIndex(index); if (index &lt; (size &gt;&gt; 1)) &#123; Node&lt;E&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; &#125; else &#123; Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; &#125;&#125; 代码较为简单，无非就是设置节点的prev和next关系。可以看到，除了头插和尾插外，在链表别的位置插入新节点，涉及到节点遍历操作，所以我们常说的链表插入速度快，指的是插入节点改变前后节点的引用过程很快。 get(int index)get(int index)获取指定下标元素： 123456789101112131415161718192021public E get(int index) &#123; checkElementIndex(index); return node(index).item;&#125;// 采用二分法遍历每个Node节点，直到找到index位置的节点Node&lt;E&gt; node(int index) &#123; // assert isElementIndex(index); if (index &lt; (size &gt;&gt; 1)) &#123; Node&lt;E&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; &#125; else &#123; Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; &#125;&#125; 代码较为简单，就是通过node函数查找指定index下标Node，然后获取其item属性值，节点查找需要遍历。 set(int index, E element)set(int index, E element)设置指定下标节点的item为指定值： 1234567891011121314151617181920212223242526272829public E set(int index, E element) &#123; // 下标合法性检查 checkElementIndex(index); // 获取index下标节点 Node&lt;E&gt; x = node(index); // 获取旧值 E oldVal = x.item; // 设置新值 x.item = element; // 返回旧值 return oldVal;&#125;// 采用二分法遍历每个Node节点，直到找到index位置的节点Node&lt;E&gt; node(int index) &#123; // assert isElementIndex(index); if (index &lt; (size &gt;&gt; 1)) &#123; Node&lt;E&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; &#125; else &#123; Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; &#125;&#125; 可以看到，set方法也需要通过遍历查找目标节点。 remove(int index)remove(int index)删除指定下标节点： 123456789101112131415161718192021222324252627282930public E remove(int index) &#123; checkElementIndex(index); return unlink(node(index));&#125;E unlink(Node&lt;E&gt; x) &#123; // assert x != null; final E element = x.item; final Node&lt;E&gt; next = x.next; final Node&lt;E&gt; prev = x.prev; if (prev == null) &#123; first = next; &#125; else &#123; prev.next = next; x.prev = null; &#125; if (next == null) &#123; last = prev; &#125; else &#123; next.prev = prev; x.next = null; &#125; x.item = null; size--; modCount++; return element;&#125; remove(int index)通过node方法找到需要删除的节点，然后调用unlink方法改变删除节点的prev和next节点的前继和后继节点。 剩下的方法可以自己阅读源码。 RandomAccess接口RandomAccess接口是一个空接口，不包含任何方法，只是作为一个标识： 1234package java.util;public interface RandomAccess &#123;&#125; 实现该接口的类说明其支持快速随机访问，比如ArrayList实现了该接口，说明ArrayList支持快速随机访问。所谓快速随机访问指的是通过元素的下标即可快速获取元素对象，无需遍历，而LinkedList则没有这个特性，元素获取必须遍历链表。 在Collections类的binarySearch(List&lt;? extends Comparable&lt;? super T&gt;&gt; list, T key)方法中，可以看到RandomAccess的应用： 1234567public static &lt;T&gt;int binarySearch(List&lt;? extends Comparable&lt;? super T&gt;&gt; list, T key) &#123; if (list instanceof RandomAccess || list.size()&lt;BINARYSEARCH_THRESHOLD) return Collections.indexedBinarySearch(list, key); else return Collections.iteratorBinarySearch(list, key);&#125; 当list实现了RandomAccess接口时，调用indexedBinarySearch方法，否则调用iteratorBinarySearch。所以当我们遍历集合时，如果集合实现了RandomAccess接口，优先选择普通for循环，其次foreach；遍历未实现RandomAccess的接口，优先选择iterator遍历。","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"}]},{"title":"HashMap底层实现原理","slug":"HashMap底层实现原理","date":"2021-05-26T13:12:30.000Z","updated":"2023-06-04T13:14:15.188Z","comments":true,"path":"2021/05/26/HashMap底层实现原理/","link":"","permalink":"http://example.com/2021/05/26/HashMap%E5%BA%95%E5%B1%82%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/","excerpt":"","text":"本节用于记录Java HashMap底层数据结构、方法实现原理等，基于JDK 1.8。 底层数据结构Java HashMap底层采用哈希表结构（数组+链表、JDK1.8后为数组+链表或红黑树）实现，结合了数组和链表的优点： 数组优点：通过数组下标可以快速实现对数组元素的访问，效率极高； 链表优点：插入或删除数据不需要移动元素，只需修改节点引用，效率极高。 HashMap图示如下所示： HashMap内部使用数组存储数据，数组中的每个元素类型为Node&lt;K,V&gt;： 123456789101112131415161718192021222324252627282930313233343536373839static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; V value; Node&lt;K,V&gt; next; Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final String toString() &#123; return key + &quot;=&quot; + value; &#125; public final int hashCode() &#123; return Objects.hashCode(key) ^ Objects.hashCode(value); &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; public final boolean equals(Object o) &#123; if (o == this) return true; if (o instanceof Map.Entry) &#123; Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) return true; &#125; return false; &#125;&#125; Node包含了四个字段：hash、key、value、next，其中next表示链表的下一个节点。 HashMap通过hash方法计算key的哈希码，然后通过(n-1)&amp;hash公式（n为数组长度）得到key在数组中存放的下标。当两个key在数组中存放的下标一致时，数据将以链表的方式存储（哈希冲突，哈希碰撞）。我们知道，在链表中查找数据必须从第一个元素开始一层一层往下找，直到找到为止，时间复杂度为O(N)，所以当链表长度越来越长时，HashMap的效率越来越低。 为了解决这个问题，JDK1.8开始采用数组+链表+红黑树的结构来实现HashMap。当链表中的元素超过8个（TREEIFY_THRESHOLD）并且数组长度大于64（MIN_TREEIFY_CAPACITY）时，会将链表转换为红黑树，转换后数据查询时间复杂度为O(logN)。 红黑树的节点使用TreeNode表示： 1234567891011static final class TreeNode&lt;K,V&gt; extends LinkedHashMap.Entry&lt;K,V&gt; &#123; TreeNode&lt;K,V&gt; parent; // red-black tree links TreeNode&lt;K,V&gt; left; TreeNode&lt;K,V&gt; right; TreeNode&lt;K,V&gt; prev; // needed to unlink next upon deletion boolean red; TreeNode(int hash, K key, V val, Node&lt;K,V&gt; next) &#123; super(hash, key, val, next); &#125; ...&#125; HashMap包含几个重要的变量： 1234567891011121314151617181920212223242526272829303132// 数组默认的初始化长度16static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4;// 数组最大容量，2的30次幂，即1073741824static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;// 默认加载因子值static final float DEFAULT_LOAD_FACTOR = 0.75f;// 链表转换为红黑树的长度阈值static final int TREEIFY_THRESHOLD = 8;// 红黑树转换为链表的长度阈值static final int UNTREEIFY_THRESHOLD = 6;// 链表转换为红黑树时，数组容量必须大于等于64static final int MIN_TREEIFY_CAPACITY = 64;// HashMap里键值对个数transient int size;// 扩容阈值，计算方法为 数组容量*加载因子int threshold;// HashMap使用数组存放数据，数组元素类型为Node&lt;K,V&gt;transient Node&lt;K,V&gt;[] table;// 加载因子final float loadFactor;// 用于快速失败，由于HashMap非线程安全，在对HashMap进行迭代时，如果期间其他线程的参与导致HashMap的结构发生变化了（比如put，remove等操作），直接抛出ConcurrentModificationException异常transient int modCount; 上面这些字段在下面源码解析的时候尤为重要，其中需要着重讨论的是加载因子是什么，为什么默认值为0.75f。 加载因子也叫扩容因子，用于决定HashMap数组何时进行扩容。比如数组容量为16，加载因子为0.75，那么扩容阈值为16*0.75=12，即HashMap数据量大于等于12时，数组就会进行扩容。我们都知道，数组容量的大小在创建的时候就确定了，所谓的扩容指的是重新创建一个指定容量的数组，然后将旧值复制到新的数组里。扩容这个过程非常耗时，会影响程序性能。所以加载因子是基于容量和性能之间平衡的结果： 当加载因子过大时，扩容阈值也变大，也就是说扩容的门槛提高了，这样容量的占用就会降低。但这时哈希碰撞的几率就会增加，效率下降； 当加载因子过小时，扩容阈值变小，扩容门槛降低，容量占用变大。这时候哈希碰撞的几率下降，效率提高。 可以看到容量占用和性能是此消彼长的关系，它们的平衡点由加载因子决定，0.75是一个即兼顾容量又兼顾性能的经验值。 此外用于存储数据的table字段使用transient修饰，通过transient修饰的字段在序列化的时候将被排除在外，那么HashMap在序列化后进行反序列化时，是如何恢复数据的呢？HashMap通过自定义的readObject/writeObject方法自定义序列化和反序列化操作。这样做主要是出于以下两点考虑： table一般不会存满，即容量大于实际键值对个数，序列化table未使用的部分不仅浪费时间也浪费空间； key对应的类型如果没有重写hashCode方法，那么它将调用Object的hashCode方法，该方法为native方法，在不同JVM下实现可能不同；换句话说，同一个键值对在不同的JVM环境下，在table中存储的位置可能不同，那么在反序列化table操作时可能会出错。 所以在HashXXX类中（如HashTable，HashSet，LinkedHashMap等等），我们可以看到，这些类用于存储数据的字段都用transient修饰，并且都自定义了readObject/writeObject方法。readObject/writeObject方法这节就不进行源码分析了，有兴趣自己研究。 put源码put方法源码如下： 123public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125; put方法通过hash函数计算key对应的哈希值，hash函数源码如下： 1234static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 如果key为null，返回0，不为null，则通过(h = key.hashCode()) ^ (h &gt;&gt;&gt; 16)公式计算得到哈希值。该公式通过hashCode的高16位异或低16位得到哈希值，主要从性能、哈希碰撞角度考虑，减少系统开销，不会造成因为高位没有参与下标计算从而引起的碰撞。 得到key对应的哈希值后，再调用putVal(hash(key), key, value, false, true)方法插入元素： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // 如果数组(哈希表)为null或者长度为0，则进行数组初始化操作 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 根据key的哈希值计算出数据插入数组的下标位置，公式为(n-1)&amp;hash if ((p = tab[i = (n - 1) &amp; hash]) == null) // 如果该下标位置还没有元素，则直接创建Node对象，并插入 tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; // 如果目标位置key已经存在，则直接覆盖 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 如果目标位置key不存在，并且节点为红黑树，则插入红黑树中 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; // 否则为链表结构，遍历链表，尾部插入 for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); // 如果链表长度大于等于TREEIFY_THRESHOLD，则考虑转换为红黑树 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); // 转换为红黑树操作，内部还会判断数组长度是否小于MIN_TREEIFY_CAPACITY，如果是的话不转换 break; &#125; // 如果链表中已经存在该key的话，直接覆盖替换 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key // 返回被替换的值 V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; // 模数递增 ++modCount; // 当键值对个数大于等于扩容阈值的时候，进行扩容操作 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; put操作过程总结： 判断HashMap数组是否为空，是的话初始化数组（由此可见，在创建HashMap对象的时候并不会直接初始化数组）； 通过(n-1) &amp; hash计算key在数组中的存放索引； 目标索引位置为空的话，直接创建Node存储； 目标索引位置不为空的话，分下面三种情况： 4.1. key相同，覆盖旧值； 4.2. 该节点类型是红黑树的话，执行红黑树插入操作； 4.3. 该节点类型是链表的话，遍历到最后一个元素尾插入，如果期间有遇到key相同的，则直接覆盖。如果链表长度大于等于TREEIFY_THRESHOLD，并且数组容量大于等于MIN_TREEIFY_CAPACITY，则将链表转换为红黑树结构； 判断HashMap元素个数是否大于等于threshold，是的话，进行扩容操作。 get源码get和put相比，就简单多了，下面是get操作源码： 1234567891011121314151617181920212223242526272829public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; // 判断数组是否为空，数组长度是否大于0，目标索引位置下元素是否为空，是的话直接返回null if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; // 如果目标索引位置元素就是要找的元素，则直接返回 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; // 如果目标索引位置元素的下一个节点不为空 if ((e = first.next) != null) &#123; // 如果类型是红黑树，则从红黑树中查找 if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); do &#123; // 否则就是链表，遍历链表查找目标元素 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; resize源码由前面的put源码分析我们知道，数组的初始化和扩容都是通过调用resize方法完成的，所以现在来关注下resize方法的源码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091final Node&lt;K,V&gt;[] resize() &#123; // 扩容前的数组 Node&lt;K,V&gt;[] oldTab = table; // 扩容前的数组的大小和阈值 int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; // 预定义新数组的大小和阈值 int newCap, newThr = 0; if (oldCap &gt; 0) &#123; // 超过最大值就不再扩容了 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; // 扩大容量为当前容量的两倍，但不能超过 MAXIMUM_CAPACITY else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; // 当前数组没有数据，使用初始化的值 else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // zero initial threshold signifies using defaults // 如果初始化的值为 0，则使用默认的初始化容量，默认值为16 newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; // 如果新的容量等于 0 if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;&quot;rawtypes&quot;,&quot;unchecked&quot;&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; // 开始扩容，将新的容量赋值给 table table = newTab; // 原数据不为空，将原数据复制到新 table 中 if (oldTab != null) &#123; // 根据容量循环数组，复制非空元素到新 table for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; // 如果链表只有一个，则进行直接赋值 if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) // 红黑树相关的操作 ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // preserve order // 链表复制，JDK 1.8 扩容优化部分 Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; // 原索引 if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; // 原索引 + oldCap else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); // 将原索引放到哈希桶中 if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; // 将原索引 + oldCap 放到哈希桶中 if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; JDK1.8在扩容时通过高位运算e.hash &amp; oldCap结果是否为0来确定元素是否需要移动，主要有如下两种情况： 情况一： 扩容前oldCap=16，hash=5，(n-1)&amp;hash=15&amp;5=5，hash&amp;oldCap=5&amp;16=0； 扩容后newCap=32，hash=5，(n-1)&amp;hash=31&amp;5=5，hash&amp;oldCap=5&amp;16=0。 这种情况下，扩容后元素索引位置不变，并且hash&amp;oldCap==0。 情况二： 扩容前oldCap=16，hash=18，(n-1)&amp;hash=15&amp;18=2，hash&amp;oldCap=18&amp;16=16； 扩容后newCap=32，hash=18，(n-1)&amp;hash=31&amp;18=18，hash&amp;oldCap=18&amp;16=16。 这种情况下，扩容后元素索引位置为18，即旧索引2加16(oldCap)，并且hash&amp;oldCap!=0。 遍历原理我们通常使用下面两种方式遍历HashMap： 123456789101112131415161718HashMap&lt;String, Object&gt; map = new HashMap&lt;&gt;();map.put(&quot;1&quot;, &quot;a&quot;);map.put(&quot;4&quot;, &quot;d&quot;);map.put(&quot;2&quot;, &quot;b&quot;);map.put(&quot;9&quot;, &quot;i&quot;);map.put(&quot;3&quot;, &quot;c&quot;);Set&lt;Map.Entry&lt;String, Object&gt;&gt; entries = map.entrySet();for (Map.Entry&lt;String, Object&gt; entry : entries) &#123; System.out.println(entry.getKey() + &quot;: &quot; + entry.getValue());&#125;System.out.println(&quot;-------&quot;);Set&lt;String&gt; keySet = map.keySet();for (String key : keySet) &#123; System.out.println(key + &quot;: &quot; + map.get(key));&#125; 程序输出： 12345678910111: a2: b3: c4: d9: i-------1: a2: b3: c4: d9: i 通过前面对put源码的分析，我们知道HashMap是无序的，输出元素顺序和插入元素顺序一般都不一样。但是多次运行上面的程序你会发现，每次遍历的顺序都是一样的。那么遍历的原理是什么，内部是如何操作的？ 通过entrySet或者keySet遍历，它们的内部原理是一样的，这里以entrySet为例。 通过查看代码对应的class文件，你会发现下面这段代码实际会被转换为iterator遍历： 1234Set&lt;Map.Entry&lt;String, Object&gt;&gt; entries = map.entrySet();for (Map.Entry&lt;String, Object&gt; entry : entries) &#123; System.out.println(entry.getKey() + &quot;: &quot; + entry.getValue());&#125; 增强for循环会被编译为： 1234567Set&lt;Entry&lt;String, Object&gt;&gt; entries = map.entrySet();Iterator var3 = entries.iterator();while(var3.hasNext()) &#123; Entry&lt;String, Object&gt; entry = (Entry)var3.next(); System.out.println((String)entry.getKey() + &quot;: &quot; + entry.getValue());&#125; 我们查看entrySet，iterator，hasNext，next方法的源码就可以清楚的了解到HashMap遍历原理了： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970public Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet() &#123; Set&lt;Map.Entry&lt;K,V&gt;&gt; es; // entrySet一开始为null，通过new EntrySet()创建 return (es = entrySet) == null ? (entrySet = new EntrySet()) : es;&#125;final class EntrySet extends AbstractSet&lt;Map.Entry&lt;K,V&gt;&gt; &#123; public final int size() &#123; return size; &#125; public final void clear() &#123; HashMap.this.clear(); &#125; // EntrySet内部包含迭代器方法，方法内部通过new EntryIterator()创建Entry迭代器 public final Iterator&lt;Map.Entry&lt;K,V&gt;&gt; iterator() &#123; return new EntryIterator(); &#125; ...... &#125;// EntryIterator继承自HashIterator，调用EntryIterator的hasNext方法实际调用的是// 父类HashIterator的hashNext方法，调用EntryIterator的next方法，方法内部调用的是父类HashIterator// 的nextNode方法，所以我们主要关注HashIterator的源码final class EntryIterator extends HashIterator implements Iterator&lt;Map.Entry&lt;K,V&gt;&gt; &#123; public final Map.Entry&lt;K,V&gt; next() &#123; return nextNode(); &#125;&#125;abstract class HashIterator &#123; Node&lt;K,V&gt; next; // 下一个节点 Node&lt;K,V&gt; current; // 当前节点 int expectedModCount; // 期待的模数值，用于快速失败 int index; // 当前遍历的table index HashIterator() &#123; // 将当前模数值赋值给期待的模数值，所以在遍历的时候，别的线程调用了当前hashMap实例的 // 增删改方法，模数值会改变，那么expectedModCount和modCount就不相等了，遍历操作直接 // 抛出ConcurrentModificationException expectedModCount = modCount; Node&lt;K,V&gt;[] t = table; current = next = null; // 从hashMap数组头部开始遍历 index = 0; if (t != null &amp;&amp; size &gt; 0) &#123; // advance to first entry // 从数组头部开始找，index递增，当index位置的节点不为空时，将其赋值给next // 也就是说，在创建hashMap迭代器的时候，内部就已经找到了hashMap数组中第一个非空节点了 do &#123;&#125; while (index &lt; t.length &amp;&amp; (next = t[index++]) == null); &#125; &#125; public final boolean hasNext() &#123; // 逻辑很简单，就是判断next是否为空 return next != null; &#125; final Node&lt;K,V&gt; nextNode() &#123; Node&lt;K,V&gt;[] t; Node&lt;K,V&gt; e = next; if (modCount != expectedModCount) // 模数判断 throw new ConcurrentModificationException(); if (e == null) // 如果next为空了，还调用nextNode方法的话，将抛出NoSuchElementException异常 throw new NoSuchElementException(); // 这段逻辑也很简单，主要包含如下两种情况： // 1. 如果当前节点的next节点为空的话，说明该节点无需进行链表遍历了（就一个节点或者已经到了链表的末尾），那么进行do while循环，直到找到hashMap数组中下一个不为空的节点 // 2. 如果当前节点的next节点不为空的话，说明该位置存在链表，那么外界在循环调用iterator的next方法时，实际就是不断调用nextNode方法遍历链表操作 if ((next = (current = e).next) == null &amp;&amp; (t = table) != null) &#123; do &#123;&#125; while (index &lt; t.length &amp;&amp; (next = t[index++]) == null); &#125; return e; &#125; ......&#125; 总之，遍历HashMap的过程就是从头查找HashMap数组中的不为空的结点，如果该结点下存在链表，则遍历该链表，遍历完链表后再找HashMap数组中下一个不为空的结点，以此进行下去直到遍历结束。 那么，如果某个结点下是红黑树结构的话，怎么遍历？其实当链表转换为红黑树时，链表节点里包含的next字段信息是保留的，所以我们依旧可以通过红黑树节点中的next字段找到下一个节点。 与JDK1.7主要区别JDK1.7 HashMap源码：https://github.com/ZhaoX/jdk-1.7-annotated/blob/master/src/java/util/HashMap.java。 数组元素类型不同JDK1.8 HashMap数组元素类型为Node&lt;K,V&gt;，JDK1.7 HashMap数组元素类型为Entry&lt;K,V&gt;： 12345678910transient Entry&lt;K,V&gt;[] table = (Entry&lt;K,V&gt;[]) EMPTY_TABLE;static class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final K key; V value; Entry&lt;K,V&gt; next; int hash; ......&#125; 实际就是换了个类名，并没有什么本质不同。 hash计算规则不同JDK1.7 hash计算规则为： 1234567891011121314final int hash(Object k) &#123; int h = hashSeed; if (0 != h &amp;&amp; k instanceof String) &#123; return sun.misc.Hashing.stringHash32((String) k); &#125; h ^= k.hashCode(); // This function ensures that hashCodes that differ only by // constant multiples at each bit position have a bounded // number of collisions (approximately 8 at default load factor). h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4);&#125; 相比于JDK1.8的hash方法，JDK1.7的hash方法的性能会稍差一点。 put操作不同JDK1.7并没有使用红黑树，如果哈希冲突后，都用链表解决。区别于JDK1.8的尾部插入，JDK1.7采用头部插入的方式： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public V put(K key, V value) &#123; // 键为null，将元素放置到table数组的0下标处 if (key == null) return putForNullKey(value); // 计算hash和数组下标索引位置 int hash = hash(key.hashCode()); int i = indexFor(hash, table.length); // 遍历链表，当key一致时，说明该key已经存在，使用新值替换旧值并返回 for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; modCount++; // 插入链表 addEntry(hash, key, value, i); return null; &#125; private V putForNullKey(V value) &#123; // 一样的，新旧值替换 for (Entry&lt;K,V&gt; e = table[0]; e != null; e = e.next) &#123; if (e.key == null) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; modCount++; // 插入到数组下标为0位置 addEntry(0, null, value, 0); return null; &#125; void addEntry(int hash, K key, V value, int bucketIndex) &#123; // 新值头部插入，原先头部变成新的头部元素的next Entry&lt;K, V&gt; e = table[bucketIndex]; table[bucketIndex] = new Entry&lt;K, V&gt;(hash, key, value, e); // 计数，扩容 if (size++ &gt;= threshold) resize(2 * table.length);&#125; 扩容操作不同JDK1.8在扩容时通过高位运算e.hash &amp; oldCap结果是否为0来确定元素是否需要移动，JDK1.7重新计算了每个元素的哈希值，按旧链表的正序遍历链表、在新链表的头部依次插入，即在转移数据、扩容后，容易出现链表逆序的情况： 1234567891011121314151617181920212223242526272829303132void resize(int newCapacity) &#123; Entry[] oldTable = table; int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return; &#125; Entry[] newTable = new Entry[newCapacity]; transfer(newTable, initHashSeedAsNeeded(newCapacity)); table = newTable; threshold = (int)Math.min(newCapacity * loadFactor, MAXIMUM_CAPACITY + 1);&#125;/** * Transfers all entries from current table to newTable. */void transfer(Entry[] newTable, boolean rehash) &#123; int newCapacity = newTable.length; for (Entry&lt;K,V&gt; e : table) &#123; while(null != e) &#123; Entry&lt;K,V&gt; next = e.next; if (rehash) &#123; e.hash = null == e.key ? 0 : hash(e.key); &#125; int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; &#125; &#125;&#125; 此时若多线程并发执行resize操作，容易出现环形链表，从而在获取数据、遍历链表时造成死循环，具体可以参考：https://blog.csdn.net/hhx0626/article/details/54024222。","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"}]},{"title":"LinkedHashMap源码解析","slug":"LinkedHashMap源码解析","date":"2021-05-26T13:12:30.000Z","updated":"2023-06-04T13:14:15.189Z","comments":true,"path":"2021/05/26/LinkedHashMap源码解析/","link":"","permalink":"http://example.com/2021/05/26/LinkedHashMap%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/","excerpt":"","text":"HashMap元素插入是无序的，为了让遍历顺序和插入顺序一致，我们可以使用LinkedHashMap，其内部维护了一个双向链表来存储元素顺序，并且可以通过accessOrder属性控制遍顺序为插入顺序或者为访问顺序。本节将记录LinkedHashMap的内部实现原理，基于JDK1.8，并且用LinkedHashMap实现一个简单的LRU。 类结构LinkedHashMap类层级关系图： LinkedHashMap继承自HashMap，大部分方法都是直接使用HashMap的。接着查看成员变量： 12345678// 双向链表的头部节点（最早插入的，年纪最大的节点）transient LinkedHashMap.Entry&lt;K,V&gt; head;// 双向链表的尾部节点（最新插入的，年纪最小的节点）transient LinkedHashMap.Entry&lt;K,V&gt; tail;// 用于控制访问顺序，为true时，按插入顺序；为false时，按访问顺序final boolean accessOrder; head和tail使用transient修饰，原因在介绍HashMap源码的时候分析过。 LinkedHashMap继承自HashMap，所以内部存储数据的方式和HashMap一样，使用数组加链表（红黑树）的结构存储数据，LinkedHashMap和HashMap相比，额外的维护了一个双向链表，用于存储节点的顺序。这个双向链表的类型为LinkedHashMap.Entry： 123456static class Entry&lt;K,V&gt; extends HashMap.Node&lt;K,V&gt; &#123; Entry&lt;K,V&gt; before, after; Entry(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; super(hash, key, value, next); &#125;&#125; LinkedHashMap.Entry类层级关系图： LinkedHashMap.Entry继承自HashMap的Node类，新增了before和after属性，用于维护前继和后继节点，以此形成双向链表。 构造函数LinkedHashMap的构造函数其实没什么特别的，就是调用父类的构造器初始化HashMap的过程，只不过额外多了初始化LinkedHashMap的accessOrder属性的操作： 123456789101112131415161718192021222324252627public LinkedHashMap(int initialCapacity, float loadFactor) &#123; super(initialCapacity, loadFactor); accessOrder = false;&#125;public LinkedHashMap(int initialCapacity) &#123; super(initialCapacity); accessOrder = false;&#125;public LinkedHashMap() &#123; super(); accessOrder = false;&#125;public LinkedHashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; super(); accessOrder = false; putMapEntries(m, false);&#125;public LinkedHashMap(int initialCapacity, float loadFactor, boolean accessOrder) &#123; super(initialCapacity, loadFactor); this.accessOrder = accessOrder;&#125; 简单使用在分析LinkedHashMap方法实现之前，我们先通过例子感受下LinkedHashMap的特性： 1234567891011LinkedHashMap&lt;String, Object&gt; map = new LinkedHashMap&lt;&gt;(16, 0.75f, false);map.put(&quot;1&quot;, &quot;a&quot;);map.put(&quot;6&quot;, &quot;b&quot;);map.put(&quot;3&quot;, &quot;c&quot;);System.out.println(map);map.get(&quot;6&quot;);System.out.println(map);map.put(&quot;4&quot;, &quot;d&quot;);System.out.println(map); 输出： 123&#123;1=a, 6=b, 3=c&#125;&#123;1=a, 6=b, 3=c&#125;&#123;1=a, 6=b, 3=c, 4=d&#125; 可以看到元素的输出顺序就是我们插入的顺序。 将accessOrder属性改为true： 123&#123;1=a, 6=b, 3=c&#125;&#123;1=a, 3=c, 6=b&#125;&#123;1=a, 3=c, 6=b, 4=d&#125; 可以看到，一开始输出&#123;1=a, 6=b, 3=c&#125;。当我们通过get方法访问key为6的键值对后，程序输出&#123;1=a, 3=c, 6=b&#125;。也就是说，当accessOrder属性为true时，元素按访问顺序排列，即最近访问的元素会被移动到双向列表的末尾。所谓的“访问”并不是只有get方法，符合“访问”一词的操作有put、putIfAbsent、get、getOrDefault、compute、computeIfAbsent、computeIfPresent和merge方法。 下面我们通过方法源码的分析就能清楚地知道LinkedHashMap是如何控制元素访问顺序的。 方法解析put(K key, V value)LinkedHashMap并没有重写put(K key, V value)方法，直接使用HashMap的put(K key, V value)方法。那么问题就来了，既然LinkedHashMap没有重写put(K key, V value)，那它是如何通过内部的双向链表维护元素顺序的？我们查看put(K key, V value)方法源码就能发现原因（因为put(K key, V value)源码在Java-HashMap底层实现原理一节中已经剖析过，所以下面我们只在和LinkedHashMap功能相关的代码上添加注释）： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125;final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; if ((p = tab[i = (n - 1) &amp; hash]) == null) // 创建节点 tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; else if (p instanceof TreeNode) // 方法内部包含newTreeNode的操作 e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; // 创建节点 p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; // 节点访问后续操作 afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; if (++size &gt; threshold) resize(); // 节点插入后续操作 afterNodeInsertion(evict); return null;&#125; newNode方法用于创建链表节点，LinkedHashMap重写了newNode方法： 123456789101112131415161718192021Node&lt;K,V&gt; newNode(int hash, K key, V value, Node&lt;K,V&gt; e) &#123; // 创建LinkedHashMap.Entry实例 LinkedHashMap.Entry&lt;K,V&gt; p = new LinkedHashMap.Entry&lt;K,V&gt;(hash, key, value, e); // 将新节点放入LinkedHashMap维护的双向链表尾部 linkNodeLast(p); return p;&#125;private void linkNodeLast(LinkedHashMap.Entry&lt;K,V&gt; p) &#123; LinkedHashMap.Entry&lt;K,V&gt; last = tail; tail = p; // 如果尾节点为空，说明双向链表是空的，所以将该节点赋值给头节点，双向链表得以初始化 if (last == null) head = p; else &#123; // 否则将该节点放到双向链表的尾部 p.before = last; last.after = p; &#125;&#125; 可以看到，对于LinkedHashMap实例，put操作内部创建的的节点类型为LinkedHashMap.Entry，除了往HashMap内部table插入数据外，还往LinkedHashMap的双向链表尾部插入了数据。 如果是往红黑树结构插入数据，那么put将调用putTreeVal方法往红黑树里插入节点，putTreeVal方法内部通过newTreeNode方法创建树节点。LinkedHashMap重写了newTreeNode方法： 1234567TreeNode&lt;K,V&gt; newTreeNode(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; // 创建TreeNode实例 TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt;(hash, key, value, next); // 将新节点放入LinkedHashMap维护的双向链表尾部 linkNodeLast(p); return p;&#125; 节点类型为TreeNode，那么这个类型是在哪里定义的呢？其实TreeNode为HashMap里定义的，查看其源码： 123456789101112static final class TreeNode&lt;K,V&gt; extends LinkedHashMap.Entry&lt;K,V&gt; &#123; TreeNode&lt;K,V&gt; parent; // red-black tree links TreeNode&lt;K,V&gt; left; TreeNode&lt;K,V&gt; right; TreeNode&lt;K,V&gt; prev; // needed to unlink next upon deletion boolean red; TreeNode(int hash, K key, V val, Node&lt;K,V&gt; next) &#123; super(hash, key, val, next); &#125; ......&#125; TreeNode继承自LinkedHashMap.Entry： 所以TreeNode也包含before和after属性，即使插入的节点类型为TreeNode，依旧可以用LinkedHashMap双向链表维护节点顺序。 在put方法中，如果插入的key已经存在的话，还会执行afterNodeAccess操作，该方法在HashMap中为空方法： 1void afterNodeAccess(Node&lt;K,V&gt; p) &#123; &#125; afterNodeAccess方法顾名思义，就是当节点被访问后执行某些操作。LinkedHashMap重写了这个方法： 1234567891011121314151617181920212223242526void afterNodeAccess(Node&lt;K,V&gt; e) &#123; // move node to last LinkedHashMap.Entry&lt;K,V&gt; last; // 如果accessOrder属性为true，并且当前节点不是双向链表的尾节点的话执行if内逻辑 if (accessOrder &amp;&amp; (last = tail) != e) &#123; // 这部分逻辑也很好理解，就是将当前节点移动到双向链表的尾部，并且改变相关节点的前继后继关系 LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; p.after = null; if (b == null) head = a; else b.after = a; if (a != null) a.before = b; else last = b; if (last == null) head = p; else &#123; p.before = last; last.after = p; &#125; tail = p; ++modCount; &#125;&#125; 所以当accessOrder为true时候，调用LinkedHashMap的put方法，插入相同key值的键值对时，该键值对会被移动到尾部： 1234567LinkedHashMap&lt;String, Object&gt; map = new LinkedHashMap&lt;&gt;(16, 0.75f, true);map.put(&quot;1&quot;, &quot;a&quot;);map.put(&quot;6&quot;, &quot;b&quot;);map.put(&quot;3&quot;, &quot;c&quot;);System.out.println(map);map.put(&quot;6&quot;, &quot;b&quot;);System.out.println(map); 程序输出： 12&#123;1=a, 6=b, 3=c&#125;&#123;1=a, 3=c, 6=b&#125; 在put方法尾部，还调用了afterNodeInsertion方法，方法顾名思义，用于插入节点后执行某些操作，该方法在HashMap中也是空方法： 1void afterNodeInsertion(boolean evict) &#123; &#125; LinkedHashMap重写了该方法： 12345678910111213141516// 这里evict为truevoid afterNodeInsertion(boolean evict) &#123; // possibly remove eldest LinkedHashMap.Entry&lt;K,V&gt; first; // 如果头部节点不为空并且removeEldestEntry返回true的话 if (evict &amp;&amp; (first = head) != null &amp;&amp; removeEldestEntry(first)) &#123; // 获取头部节点的key K key = first.key; // 调用父类HashMap的removeNode方法，删除节点 removeNode(hash(key), key, null, false, true); &#125;&#125;protected boolean removeEldestEntry(Map.Entry&lt;K,V&gt; eldest) &#123; // 在LinkedHashMap中，该方法永远返回false return false;&#125; 基于这个特性，我们可以通过继承LinkedHashMap的方式重写removeEldestEntry方法，以此实现LRU，下面再做实现。 你可能会问，removeNode删除的是HashMap的table中的节点，那么用于维护节点顺序的双向链表不是也应该删除头部节点吗？为什么上面代码没有看到这部分操作？其实当你查看removeNode方法的源码就能看到这部分操作了： 1234567891011121314151617181920212223242526272829303132333435363738394041final Node&lt;K,V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, index; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (p = tab[index = (n - 1) &amp; hash]) != null) &#123; Node&lt;K,V&gt; node = null, e; K k; V v; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) node = p; else if ((e = p.next) != null) &#123; if (p instanceof TreeNode) node = ((TreeNode&lt;K,V&gt;)p).getTreeNode(hash, key); else &#123; do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; node = e; break; &#125; p = e; &#125; while ((e = e.next) != null); &#125; &#125; if (node != null &amp;&amp; (!matchValue || (v = node.value) == value || (value != null &amp;&amp; value.equals(v)))) &#123; if (node instanceof TreeNode) ((TreeNode&lt;K,V&gt;)node).removeTreeNode(this, tab, movable); else if (node == p) tab[index] = node.next; else p.next = node.next; ++modCount; --size; // 节点删除后，执行后续操作 afterNodeRemoval(node); return node; &#125; &#125; return null;&#125; afterNodeRemoval方法顾名思义，用于节点删除后执行后续操作。该方法在HashMap中为空方法： 1void afterNodeRemoval(Node&lt;K,V&gt; p) &#123; &#125; LinkedHashMap重写了该方法： 1234567891011121314// 改变节点的前继后继引用void afterNodeRemoval(Node&lt;K,V&gt; e) &#123; // unlink LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; p.before = p.after = null; if (b == null) head = a; else b.after = a; if (a == null) tail = b; else a.before = b;&#125; 通过该方法，我们就从LinkedHashMap的双向链表中删除了头部结点。 其实通过put方法我们就已经搞清楚了LinkedHashMap内部是如何通过双向链表维护键值对顺序的，但为了让文章更饱满一点，下面继续分析几个方法源码。 get(Object key)LinkedHashMap重写了HashMap的get方法： 123456789public V get(Object key) &#123; Node&lt;K,V&gt; e; if ((e = getNode(hash(key), key)) == null) return null; // 多了这一步操作，当accessOrder属性为true时，将key对应的键值对节点移动到双向列表的尾部 if (accessOrder) afterNodeAccess(e); return e.value;&#125; remove(Object key)LinkedHashMap没有重写remove方法，查看HashMap的remove方法： 1234567public V remove(Object key) &#123; Node&lt;K,V&gt; e; // 调用removeNode删除节点，removeNode方法内部调用了afterNodeRemoval方法，上面介绍put // 方法时分析过了，所以不再赘述 return (e = removeNode(hash(key), key, null, false, true)) == null ? null : e.value;&#125; 迭代器既然LinkedHashMap内部通过双向链表维护键值对顺序的话，那么我们可以猜测遍历LinkedHashMap实际就是遍历LinkedHashMap维护的双向链表： 查看LinkedHashMap类entrySet方法的实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet() &#123; Set&lt;Map.Entry&lt;K,V&gt;&gt; es; // 创建LinkedEntrySet return (es = entrySet) == null ? (entrySet = new LinkedEntrySet()) : es;&#125;final class LinkedEntrySet extends AbstractSet&lt;Map.Entry&lt;K,V&gt;&gt; &#123; public final int size() &#123; return size; &#125; public final void clear() &#123; LinkedHashMap.this.clear(); &#125; public final Iterator&lt;Map.Entry&lt;K,V&gt;&gt; iterator() &#123; // 迭代器类型为LinkedEntryIterator return new LinkedEntryIterator(); &#125; ......&#125;// LinkedEntryIterator继承自LinkedHashIteratorfinal class LinkedEntryIterator extends LinkedHashIterator implements Iterator&lt;Map.Entry&lt;K,V&gt;&gt; &#123; // next方法内部调用LinkedHashIterator的nextNode方法 public final Map.Entry&lt;K,V&gt; next() &#123; return nextNode(); &#125;&#125;abstract class LinkedHashIterator &#123; LinkedHashMap.Entry&lt;K,V&gt; next; LinkedHashMap.Entry&lt;K,V&gt; current; int expectedModCount; LinkedHashIterator() &#123; // 初始化时，将双向链表的头部节点赋值给next，说明遍历LinkedHashMap是从 // LinkedHashMap的双向链表头部开始的 next = head; // 同样也有快速失败的特性 expectedModCount = modCount; current = null; &#125; public final boolean hasNext() &#123; return next != null; &#125; final LinkedHashMap.Entry&lt;K,V&gt; nextNode() &#123; LinkedHashMap.Entry&lt;K,V&gt; e = next; if (modCount != expectedModCount) throw new ConcurrentModificationException(); if (e == null) throw new NoSuchElementException(); current = e; // 不断获取当前节点的after节点，遍历 next = e.after; return e; &#125; ......&#125; 上述代码符合我们的猜测。 LRU简单实现LRU（Least Recently Used）指的是最近最少使用，是一种缓存淘汰算法，哪个最近不怎么用了就淘汰掉。 我们知道LinkedHashMap内的removeEldestEntry方法固定返回false，并不会执行元素删除操作，所以我们可以通过继承LinkedHashMap，重写removeEldestEntry方法来实现LRU。 假如我们现在有如下需求： 用LinkedHashMap实现缓存，缓存最多只能存储5个元素，当元素个数超过5的时候，删除（淘汰）那些最近最少使用的数据，仅保存热点数据。 新建LRUCache类，继承LinkedHashMap： 12345678910111213141516171819202122232425262728293031public class LRUCache&lt;K, V&gt; extends LinkedHashMap&lt;K, V&gt; &#123; /** * 缓存允许的最大容量 */ private final int maxSize; public LRUCache(int initialCapacity, int maxSize) &#123; // accessOrder必须为true super(initialCapacity, 0.75f, true); this.maxSize = maxSize; &#125; @Override protected boolean removeEldestEntry(Map.Entry&lt;K, V&gt; eldest) &#123; // 当键值对个数超过最大容量时，返回true，触发删除操作 return size() &gt; maxSize; &#125; public static void main(String[] args) &#123; LRUCache&lt;String, String&gt; cache = new LRUCache&lt;&gt;(5, 5); cache.put(&quot;1&quot;, &quot;a&quot;); cache.put(&quot;2&quot;, &quot;b&quot;); cache.put(&quot;3&quot;, &quot;c&quot;); cache.put(&quot;4&quot;, &quot;d&quot;); cache.put(&quot;5&quot;, &quot;e&quot;); cache.put(&quot;6&quot;, &quot;f&quot;); System.out.println(cache); &#125;&#125; 程序输出如下： 1&#123;2=b, 3=c, 4=d, 5=e, 6=f&#125; 可以看到最早插入的1=a已经被删除了。 通过LinkedHashMap实现LRU还是挺常见的，比如logback框架的LRUMessageCache： 1234567891011121314151617181920212223242526272829303132333435363738394041424344class LRUMessageCache extends LinkedHashMap&lt;String, Integer&gt; &#123; private static final long serialVersionUID = 1L; final int cacheSize; LRUMessageCache(int cacheSize) &#123; super((int) (cacheSize * (4.0f / 3)), 0.75f, true); if (cacheSize &lt; 1) &#123; throw new IllegalArgumentException(&quot;Cache size cannot be smaller than 1&quot;); &#125; this.cacheSize = cacheSize; &#125; int getMessageCountAndThenIncrement(String msg) &#123; // don&#x27;t insert null elements if (msg == null) &#123; return 0; &#125; Integer i; // LinkedHashMap is not LinkedHashMap. See also LBCLASSIC-255 synchronized (this) &#123; i = super.get(msg); if (i == null) &#123; i = 0; &#125; else &#123; i = i + 1; &#125; super.put(msg, i); &#125; return i; &#125; // called indirectly by get() or put() which are already supposed to be // called from within a synchronized block protected boolean removeEldestEntry(Map.Entry eldest) &#123; return (size() &gt; cacheSize); &#125; @Override synchronized public void clear() &#123; super.clear(); &#125;&#125;","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"}]},{"title":"MySQL表连接及其优化","slug":"MySQL表连接及其优化","date":"2021-05-26T13:12:30.000Z","updated":"2023-06-04T13:14:15.190Z","comments":true,"path":"2021/05/26/MySQL表连接及其优化/","link":"","permalink":"http://example.com/2021/05/26/MySQL%E8%A1%A8%E8%BF%9E%E6%8E%A5%E5%8F%8A%E5%85%B6%E4%BC%98%E5%8C%96/","excerpt":"","text":"导读： 在做MySQL数据库的优化工作时，如果只涉及到单表查询，那么95%的慢SQL都只需从索引上入手优化即可，通过添加索引来消除全表扫描或者排序操作，大概率能实现SQL语句执行速度质的飞跃。对于单表的优化操作，相信大部分DBA甚至开发人员都可以完成。 然而，在实际生产中，除了单表操作，更多的是多个表联合起来查询，这样的查询通常是慢SQL的重灾区，查询速度慢，使用服务器资源较多，高CPU，高I/O。本文通过对表连接的表现形式以及内部理论进行探究，以及思考如何优化表连接操作。 本文基于MySQL 5.7版本进行探究，由于MySQL 8中引入了新的连接方式hash join，本文可能不适用MySQL8版本 （一）MySQL的七种连接方式介绍 在MySQL中，常见的表连接方式有4类，共计7种方式： INNER JOIN：inner join是根据表连接条件，求取2个表的数据交集； LEFT JOIN ：left join是根据表连接条件，求取2个表的数据交集再加上左表剩下的数据；此外，还可以使用where过滤条件求左表独有的数据。 RIGHT JOIN：right join是根据表连接条件，求取2个表的数据交集再加上右表剩下的数据；此外，还可以使用where过滤条件求右表独有的数据。 FULL JOIN：full join是左连接与右连接的并集，MySQL并未提供full join语法，如果要实现full join,需要left join与right join进行求并集，此外还可以使用where查看2个表各自独有的数据。 通过图形来表现，各种连接形式的求取集合部分如下，蓝色部分代表满足join条件的数据： 接下来，我们通过例子来理解各种JOIN的含义。 首先创建测试数据： 12345678910111213141516-- 1.创建部门表-- 部门表记录部门信息，公司共有4个部门：财务(FINANCE)、人力(HR)、销售(SALES)、研发(RD)。-- 不一定每个部门都有人，例如，公司虽然有研发部，但是没有在编人员create table dept (deptno int,dname varchar(14),loc varchar(20));insert into dept values(10,&#x27;FINANCE&#x27;,&#x27;BEIJING&#x27;);insert into dept values(20,&#x27;HR&#x27;,&#x27;BEIJING&#x27;);insert into dept values(30,&#x27;SALES&#x27;,&#x27;SHANGHAI&#x27;);insert into dept values(40,&#x27;RD&#x27;,&#x27;CHENGDU&#x27;);-- 2.创建员工表-- 员工表记录了员工工号、姓名、部门编号。-- 不一定每个员工都有部门。例如，外包人员dd就没有部门create table emp (empno int,ename varchar(14),deptno int);insert into emp values(1,&#x27;aa&#x27;,10);insert into emp values(2,&#x27;bb&#x27;,20);insert into emp values(3,&#x27;cc&#x27;,30);insert into emp values(4,&#x27;dd&#x27;,null);insert into emp values(5,&#x27;ee&#x27;,30);insert into emp values(6,&#x27;ff&#x27;,20); ER图如下： （1.1）INNER JOIN业务场景：查看公司正式员工的详细信息，包括工号、姓名、部门名称。 需求分析：正式员工都有对应部门，使用INNER JOIN，通过部门编号关联部门与员工求交集。 SQL语句： 123456789101112mysql&gt; select e.empno,e.ename,d.dnamefrom emp e inner join dept don e.deptno = d.deptno;+-------+-------+---------+| empno | ename | dname |+-------+-------+---------+| 1 | aa | FINANCE || 2 | bb | HR || 3 | cc | SALES || 5 | ee | SALES || 6 | ff | HR |+-------+-------+---------+ INNER JOIN就是求取2个表的共有数据(交集)，我们可以这样来理解表INNER JOIN过程： 从驱动表按顺序数据，然后到被驱动表中逐行进行比较 如果条件满足，则取出该行数据(注意取出的是2个表连接之后的数据)，如果条件不满足，则丢弃数据，然后继续向下比较，直到遍历完被驱动表的所有行 一致循环上面2步，知道步骤1的驱动表也遍历结束。 对于上面SQL，其执行过程我们可以使用伪代码来描述： 12345678910// 特别注意：2个for循环，哪个表用来做外部循环，哪个表用来做内部循环，是由执行计划决定的，可用explain来查看，通常使用结果集较小的表来做驱动表，// 本例子中，SQL中顺序为emp，dept，但在执行计划中却是dept,emp。因此内外表顺序需要看MySQL的执行计划for (i=1;i&lt;=d.counts;i++)&#123; for (j=1;j&lt;=e.counts;j++&gt;) &#123; if (d[i].key = e[j].key) &#123; return d[i].dname,e[j].empno,e[j].ename; &#125; &#125;&#125; （1.2）LEFT JOIN业务场景：查看每一个部门的详细信息，包括工号、姓名、部门名称。 需求分析：既然包含每一个部门，那么可以使用部门表进行LEFT JOIN，通过部门编号关联部门与员工求交集。 SQL语句： 12345678910111213mysql&gt; select d.dname,e.empno,e.enamefrom dept d left join emp eon e.deptno = d.deptno;+---------+-------+-------+| dname | empno | ename |+---------+-------+-------+| FINANCE | 1 | aa || HR | 2 | bb || SALES | 3 | cc || SALES | 5 | ee || HR | 6 | ff || RD | NULL | NULL |+---------+-------+-------+ LEFT JOIN就是求取2个表的共有数据(交集)再加上左表剩下的数据，也就是左表的数据全部都要，左表的数据只要满足关联条件的。 我们可以这样来理解表LEFT JOIN过程： 从左表按顺序数据，然后到右表中逐行进行比较 如果条件满足，则取出该行数据(注意取出的是2个表连接之后的数据)，如果条件不满足，则丢弃数据，然后继续向下比较，直到遍历完被驱动表的所有行，如果遍历完右表所有的行都没有与左表匹配的数据，则返回左表的行，右表的记录用NULL填充。 一致循环上面2步，知道步骤1的驱动表也遍历结束。 对于上面SQL，其执行过程我们可以使用伪代码来描述： 1234567891011// 特别注意：2个for循环，哪个表用来做外部循环，哪个表用来做内部循环，是由执行计划决定的，可用explain来查看，通常使用结果集较小的表来做驱动表，// 本例子中，SQL中顺序为emp，dept，但在执行计划中却是dept,emp。因此内外表顺序需要看MySQL的执行计划for (i=1;i&lt;=d.counts;i++)&#123; for (j=1;j&lt;=e.counts;j++&gt;) &#123; if (d[i].key = e[j].key) &#123; return d[i].dname,e[j].empno,e[j].ename; &#125; &#125;&#125; 关于外连接查询算法描述(https://dev.mysql.com/doc/refman/5.7/en/nested-join-optimization.html)：通常，对于外部联接操作中第一个内部表的任何嵌套循环，都会引入一个标志，该标志在循环之前关闭并在循环之后检查。当针对外部表中的当前行找到表示内部操作数的表中的匹配项时，将打开该标志。如果在循环周期结束时该标志仍处于关闭状态，则未找到外部表的当前行的匹配项。在这种情况下，该行由NULL内部表的列的值补充 。结果行将传递到输出的最终检查项或下一个嵌套循环，但前提是该行满足所有嵌入式外部联接的联接条件。 12345678910111213141516*/for (i=1;i&lt;=d.counts;i++)&#123; var is_success=false; // 确认d.[i]是否匹配到至少1行数据，默认未匹配到 for (j=1;j&lt;=e.counts;j++&gt;) &#123; if (d[i].key = e[j].key) &#123; return d[i].dname,e[j].empno,e[j].ename; is_success = true; &#125; &#125; if (is_success=false) // 如果左边的表没有匹配到数据，也会将左边表返回，右边表用null代替 &#123; return d[i].key,null,null; &#125;&#125; LEFT JOIN的补充：使用LEFT JOIN来获取左表独有的数据 业务场景：查看哪些部门没有员工 需求分析：要查看没有部门的员工，只需要先查出所有的部门与员工关系数据，然后过滤掉有员工的数据。 SQL语句： 123456789mysql&gt; select d.dname,e.empno,e.enamefrom dept d left join emp eon d.deptno = e.deptnowhere e.deptno is null;+-------+-------+-------+| dname | empno | ename |+-------+-------+-------+| RD | NULL | NULL |+-------+-------+-------+ 使用LEFT JOIN获取2个表的共有数据(交集)再加上左表剩下的数据，然后又把交集去除。 （1.3）RIG3HT JOIN业务场景：查看每一个员工的详细信息，包括工号、姓名、部门名称。 需求分析：既然包含每一个员工，那么可以使用部门表进行LEFT JOIN，通过部门编号关联部门与员工求交集。 SQL语句： 12345678910111213mysql&gt; select d.dname,e.empno,e.enamefrom dept d right join emp eon e.deptno = d.deptno;+---------+-------+-------+| dname | empno | ename |+---------+-------+-------+| FINANCE | 1 | aa || HR | 2 | bb || HR | 6 | ff || SALES | 3 | cc || SALES | 5 | ee || NULL | 4 | dd |+---------+-------+-------+ 需要注意的是，右连接和左连接是可以相互转换的，即右连接的语句，通过调换表位置并修改连接关键字为左连接，即可实现等价转换。上面的SQL的等价左连接为： 12345678910111213mysql&gt; select d.dname,e.empno,e.enamefrom emp e left join dept don e.deptno = d.deptno;+---------+-------+-------+| dname | empno | ename |+---------+-------+-------+| FINANCE | 1 | aa || HR | 2 | bb || HR | 6 | ff || SALES | 3 | cc || SALES | 5 | ee || NULL | 4 | dd |+---------+-------+-------+ 实际上，MySQL在解析SQL阶段，会自动将右外连接转换等效的左外连接（文档：https://dev.mysql.com/doc/refman/5.7/en/outer-join-simplification.html），所以我们也无需深入的去了解右连接。 （1.4）FULL JOIN业务场景：查看所有部门及其所有员工的详细信息，包括工号、姓名、部门名称。 需求分析：既然包含每一个部门及所有员工，那么可以使用全连接获取数据。然而，MySQL并没有关键字去获取全连接的数据，我们可以通过合并左连接 SQL语句： 123456789101112131415161718mysql&gt; select d.dname,e.empno,e.enamefrom dept d left join emp eon e.deptno = d.deptnounionselect d.dname,e.empno,e.enamefrom dept d right join emp eon e.deptno = d.deptno;+---------+-------+-------+| dname | empno | ename |+---------+-------+-------+| FINANCE | 1 | aa || HR | 2 | bb || SALES | 3 | cc || SALES | 5 | ee || HR | 6 | ff || RD | NULL | NULL || NULL | 4 | dd |+---------+-------+-------+ FULL JOIN的补充： 如果要查找没有员工的部门或者没有部门的员工，即求取两个表各自独有的数据 SQL语句： 123456789101112131415mysql&gt; select d.dname,e.empno,e.enamefrom dept d left join emp eon e.deptno = d.deptnowhere e.deptno is nullunionselect d.dname,e.empno,e.enamefrom dept d right join emp eon e.deptno = d.deptnowhere d.deptno is null;+-------+-------+-------+| dname | empno | ename |+-------+-------+-------+| RD | NULL | NULL || NULL | 4 | dd |+-------+-------+-------+ （二）MySQL Join算法在MySQL 5.7中，MySQL仅支持Nested-Loop Join算法及其改进型Block-Nested-Loop Join算法，在8.0版本中，又新增了Hash Join算法，这里只讨论5.7版本的表连接方式。 （2.1）Nested-Loop Join算法嵌套循环连接算法(NLJ)从第一个循环的表中读取1行数据，并将该行传递到下一个表进行连接运算，如果符合条件，则继续与下一个表的行数据进行连接，知道连接完所有的表，然后重复上面的过程。简单来讲Nested-Loop Join就是编程中的多层for循环。假设存在3个表进行连接，连接方式如下： table join type t1 ranget2 reft3 ALL 如果使用NLJ算法进行连接，伪代码如下： 1234567for each row in t1 matching range &#123; for each row in t2 matching reference key &#123; for each row in t3 &#123; if row satisfies join conditions, send to client &#125; &#125;&#125; （2.2）Block Nested-Loop Join算法块嵌套循环（BLN）连接算法使用外部表的行缓冲来减少对内部表的读次数。例如，将外部表的10行数据读入缓冲区并将缓冲区传递到下一个内部循环，则可以将内部循环中的每一行与缓冲区的10行数据进行比较，此时，内部表读取的次数将减少为1/10。 如果使用BNL算法，上述连接的伪代码可以写为： 123456789101112131415161718192021for each row in t1 matching range &#123; for each row in t2 matching reference key &#123; store used columns from t1, t2 in join buffer if buffer is full &#123; for each row in t3 &#123; for each t1, t2 combination in join buffer &#123; if row satisfies join conditions, send to client &#125; &#125; empty join buffer &#125; &#125;&#125;if buffer is not empty &#123; for each row in t3 &#123; for each t1, t2 combination in join buffer &#123; if row satisfies join conditions, send to client &#125; &#125;&#125; MySQL Join Buffer有如下特点： join buffer可以被使用在表连接类型为ALL，index,range。换句话说，只有索引不可能被使用，或者索引全扫描，索引范围扫描等代价较大的查询才会使用Block Nested-Loop Join算法； 仅仅用于连接的列数据才会被存在连接缓存中，而不是整行数据 join_buffer_size系统变量用来决定每一个join buffer的大小 MySQL为每一个可以被缓存的join语句分配一个join buffer，以便每一个查询都可以使用join buffer。 在执行连接之前分配连接缓冲区，并在查询完成后释放连接缓冲区。 （三）表连接顺序在关系型数据库中，对于多表连接，位于嵌套循环外部的表我们称为驱动表，位于嵌套循环内部的表我们称为被驱动表，驱动表与被驱动表的顺序对于Join性能影响非常大，接下来我们探索一下MySQL中表连接的顺序。因为RIGHT JOIN和FULL JOIN在MySQL中最终都会转换为LEFT JOIN，所以我们只需讨论INNER JOIN和LEFT JOIN即可。 这里为了确保测试准确，我们使用MySQL提供的测试数据库employees，下载地址为：https://github.com/datacharmer/test_db。其ER图如下： （3.1）3INNER JOIN 对应INNER JOIN，MySQL永远选择结果集小的表作为驱动表。 例子1：查看员工部门对应信息 1234-- 将employees，dept_manager ， departments 3个表进行内连接即可select e.emp_no,e.first_name,e.last_name,d.dept_namefrom employees e inner join dept_manager dm on e.emp_no = dm.emp_noinner join departments d on dm.dept_no = d.dept_no; 我们来看一下3个表的大小，需要注意的是，这里仅仅是MySQL粗略统计行数，在这个例子中，实际行数与之有一定的差距： 1234567+--------------+------------+| table_name | table_rows |+--------------+------------+| departments | 9 || dept_manager | 24 || employees | 299468 |+--------------+------------+ 最终的执行计划为： 1234567+----+-------------+-------+------------+--------+-----------------+-----------+---------+---------------------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+--------+-----------------+-----------+---------+---------------------+------+----------+-------------+| 1 | SIMPLE | d | NULL | index | PRIMARY | dept_name | 42 | NULL | 9 | 100.00 | Using index || 1 | SIMPLE | dm | NULL | ref | PRIMARY,dept_no | dept_no | 4 | employees.d.dept_no | 2 | 100.00 | Using index || 1 | SIMPLE | e | NULL | eq_ref | PRIMARY | PRIMARY | 4 | employees.dm.emp_no | 1 | 100.00 | NULL |+----+-------------+-------+------------+--------+-----------------+-----------+---------+---------------------+------+----------+-------------+ 可以看到，在INNER JOIN中，MySQL并不是按照语句中表的出现顺序来按顺序执行的，而是首先评估每个表结果集的大小，选择小的作为驱动表，大的作为被驱动表，不管我们如何调整SQL中的表顺序，MySQL优化器选择表的顺序与上面相同。 这里需要特别说明的是：通常我们所说的”小表驱动大表”是非常不严谨的，在INNER JOIN中，MySQL永远选择结果集小的表作为驱动表，而不是小表。这有什么区别呢？结果集是指表进行了数据过滤后形成的临时表，其数据量小于或等于原表。下面提及的”小表和大表”都是指结果集大小。 例子2：查看工号为110567的员工部门对应信息 123select e.emp_no,e.first_name,e.last_name,d.dept_namefrom employees e inner join dept_manager dm on e.emp_no = dm.emp_no and e.emp_no = 110567inner join departments d on dm.dept_no = d.dept_no; 最终的执行计划为： 1234567+----+-------------+-------+------------+--------+-----------------+---------+---------+----------------------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+--------+-----------------+---------+---------+----------------------+------+----------+-------------+| 1 | SIMPLE | e | NULL | const | PRIMARY | PRIMARY | 4 | const | 1 | 100.00 | NULL || 1 | SIMPLE | dm | NULL | ref | PRIMARY,dept_no | PRIMARY | 4 | const | 1 | 100.00 | Using index || 1 | SIMPLE | d | NULL | eq_ref | PRIMARY | PRIMARY | 4 | employees.dm.dept_no | 1 | 100.00 | NULL |+----+-------------+-------+------------+--------+-----------------+---------+---------+----------------------+------+----------+-------------+ 可以看到，这里驱动表是employees，这个表是数据量最大的表，但是为什么选择它作为驱动表呢？因为他的结果集最小，在执行查询时，MySQL会首先选择employees表中emp_no=110567的数据，而这样的数据只有1条，其结果集也就最小，所以优化器选择了employees作为驱动表。 （3.2）LEFT JOIN 对于LEFT JOIN，执行顺序永远是从左往右，我们可以通过例子来看一下。 例子2：LEFT JOIN表顺序的选择测试 1234567891011121314151617181920212223242526272829303132333435-- 表顺序：e --&gt; dm --&gt; dmysql&gt; explain select e.emp_no,e.first_name,e.last_name,d.dept_namefrom employees e left join dept_manager dm on e.emp_no = dm.emp_noleft join departments d on dm.dept_no = d.dept_no;+----+-------------+-------+------------+--------+---------------+---------+---------+----------------------+--------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+--------+---------------+---------+---------+----------------------+--------+----------+-------------+| 1 | SIMPLE | e | NULL | ALL | NULL | NULL | NULL | NULL | 299468 | 100.00 | NULL || 1 | SIMPLE | dm | NULL | ref | PRIMARY | PRIMARY | 4 | employees.e.emp_no | 1 | 100.00 | Using index || 1 | SIMPLE | d | NULL | eq_ref | PRIMARY | PRIMARY | 4 | employees.dm.dept_no | 1 | 100.00 | NULL |+----+-------------+-------+------------+--------+---------------+---------+---------+----------------------+--------+----------+-------------+-- 表顺序：dm --&gt; e --&gt; dmysql&gt; explain select e.emp_no,e.first_name,e.last_name,d.dept_namefrom dept_manager dm left join employees e on e.emp_no = dm.emp_noleft join departments d on dm.dept_no = d.dept_no;+----+-------------+-------+------------+--------+---------------+---------+---------+----------------------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+--------+---------------+---------+---------+----------------------+------+----------+-------------+| 1 | SIMPLE | dm | NULL | index | NULL | dept_no | 4 | NULL | 24 | 100.00 | Using index || 1 | SIMPLE | e | NULL | eq_ref | PRIMARY | PRIMARY | 4 | employees.dm.emp_no | 1 | 100.00 | NULL || 1 | SIMPLE | d | NULL | eq_ref | PRIMARY | PRIMARY | 4 | employees.dm.dept_no | 1 | 100.00 | NULL |+----+-------------+-------+------------+--------+---------------+---------+---------+----------------------+------+----------+-------------+-- 表顺序：e --&gt; dm --&gt; dmysql&gt; explain select e.emp_no,e.first_name,e.last_name,d.dept_namefrom employees e left join dept_manager dm on e.emp_no = dm.emp_no left join departments d on dm.dept_no = d.dept_no;+----+-------------+-------+------------+--------+---------------+---------+---------+----------------------+--------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+--------+---------------+---------+---------+----------------------+--------+----------+-------------+| 1 | SIMPLE | e | NULL | ALL | NULL | NULL | NULL | NULL | 299468 | 100.00 | NULL || 1 | SIMPLE | dm | NULL | ref | PRIMARY | PRIMARY | 4 | employees.e.emp_no | 1 | 100.00 | Using index || 1 | SIMPLE | d | NULL | eq_ref | PRIMARY | PRIMARY | 4 | employees.dm.dept_no | 1 | 100.00 | NULL |+----+-------------+-------+------------+--------+---------------+---------+---------+----------------------+--------+----------+-------------+ 如果右表存在谓词过滤条件，MySQL会将left join转换为inner join，详见本文：（5.3）left join优化 （四）ON和WHERE的思考 在表连接中，我们可以在2个地方写过滤条件，一个是在ON后面，另一个就是WHERE后面了。那么，这两个地方写谓词过滤条件有什么区别呢？我们还是通过INNER JOIN和LEFT JOIN分别看一下。 （4.1）INNER JOIN 使用INNER JOIN，不管谓词条件写在ON部分还是WHERE部分，其结果都是相同的。 12345678910111213141516171819202122232425262728293031323334-- 将过滤条件写在ON部分mysql&gt; select e.empno,e.ename,d.dnamefrom emp e inner join dept don e.deptno = d.deptno and d.dname = &#x27;HR&#x27;;+-------+-------+-------+| empno | ename | dname |+-------+-------+-------+| 2 | bb | HR || 6 | ff | HR |+-------+-------+-------+-- 将过滤条件写在WHERE部分mysql&gt; select e.empno,e.ename,d.dnamefrom emp e inner join dept don e.deptno = d.deptno where d.dname = &#x27;HR&#x27;;+-------+-------+-------+| empno | ename | dname |+-------+-------+-------+| 2 | bb | HR || 6 | ff | HR |+-------+-------+-------+-- 使用非标准写法，将表连接条件和过滤条件写在WHERE部分mysql&gt; select e.empno,e.ename,d.dnamefrom emp e inner join dept dwhere e.deptno = d.deptno and d.dname = &#x27;HR&#x27;;+-------+-------+-------+| empno | ename | dname |+-------+-------+-------+| 2 | bb | HR || 6 | ff | HR |+-------+-------+-------+ 实际上，通过trace报告可以看到，在inner join中，不管谓词条件写在ON部分还是WHERE部分，MySQL都会将SQL语句的谓词条件等价改写到where后面。 （4.2）LEF3T JOIN 我们继续来看LEFT JOIN中ON与WHERE的区别。 使用ON作为谓词过滤条件： 12345678910111213mysql&gt; select e.empno,e.ename,d.dnamefrom emp e left join dept don e.deptno = d.deptno and d.dname = &#x27;HR&#x27;;+-------+-------+-------+| empno | ename | dname |+-------+-------+-------+| 1 | aa | NULL || 2 | bb | HR || 3 | cc | NULL || 4 | dd | NULL || 5 | ee | NULL || 6 | ff | HR |+-------+-------+-------+ 我们可以把使用ON的情况用下图来描述，先使用ON条件进行关联，并在关联的时候进行数据过滤： 再看看使用where的结果： 12345678910mysql&gt; select e.empno,e.ename,d.dnamefrom emp e left join dept don e.deptno = d.deptno where d.dname = &#x27;HR&#x27;;+-------+-------+-------+| empno | ename | dname |+-------+-------+-------+| 2 | bb | HR || 6 | ff | HR |+-------+-------+-------+ 我们可以把使用where的情况用下图来描述，先使用ON条件进行关联，然后对关联的结果进行数据过滤： 可以看到，在LEFT JOIN中，过滤条件放在ON和WHERE之后结果是不同的： 如果过滤条件在ON后面，那么将使用左表与右表每行数据进行连接，然后根据过滤条件判断，如果满足判断条件，则左表与右表数据进行连接，如果不满足判断条件，则返回左表数据，右表数据用NULL值代替； 如果过滤条件在WHERE后面，那么将使用左表与右表每行数据进行连接，然后将连接的结果集进行条件判断，满足条件的行信息保留。 （五）JOIN1优化JOIN语句相对而言比较复杂，我们根据SQL语句的结构考虑优化方法，JOIN相关的主要SQL结构如下： inner join inner join + 排序(group by 或者 order by) left join （5.1）inner join3优化常规inner join的SQL语法如下： 123SELECT &lt;select_list&gt;FROM &lt;left_table&gt; inner join &lt;right_table&gt; ON &lt;join_condition&gt;WHERE &lt;where_condition&gt; 优化方法： 1.对于inner join，通常是采用小表驱动大表的方式，即小标作为驱动表，大表作为被驱动表（相当于小表位于for循环的外层，大表位于for循环的内层）。这个过程MySQL数据局优化器以帮助我们完成，通常无需手动处理(特殊情况，表的统计信息不准确)。注意，这里的“小表”指的是结果集小的表。 2.对于inner join，需要对被驱动表的连接条件创建索引 3.对于inner join，考虑对连接条件和过滤条件(ON、WHERE)创建复合索引 例子1：对于inner join，需要对被驱动表的连接条件创建索引 123456789101112131415161718192021222324252627282930313233343536373839-- ---------- 构造测试表 -------------------------- -- 创建新表employees_newmysql&gt; create table employees_new like employees;Query OK, 0 rows affected (0.01 sec)mysql&gt; insert into empployees_new select * from employees;Query OK, 300024 rows affected (2.69 sec)Records: 300024 Duplicates: 0 Warnings: 0-- 创建新表salaries_newmysql&gt; create table salaries_new like salaries;Query OK, 0 rows affected (0.01 sec)mysql&gt; insert into salaries_new select * from salaries;Query OK, 2844047 rows affected (13.00 sec)Records: 2844047 Duplicates: 0 Warnings: 0-- 删除主键mysql&gt; alter table employees_new drop primary key;Query OK, 300024 rows affected (1.84 sec)Records: 300024 Duplicates: 0 Warnings: 0mysql&gt; alter table salaries_new drop primary key;Query OK, 2844047 rows affected (9.58 sec)Records: 2844047 Duplicates: 0 Warnings: 0-- 表大小mysql&gt; select table_name,table_rows from information_schema.tables a where a.table_schema = &#x27;employees&#x27;and a.table_name in (&#x27;employees_new&#x27;,&#x27;salaries_new&#x27;);+---------------+------------+| table_name | table_rows |+---------------+------------+| employees_new | 299389 || salaries_new | 2837194 |+---------------+------------+ 此时测试表ER关系如下： 进行表连接查询，语句如下： 123select e.emp_no,e.first_name,e.last_name,s.salary,s.from_date,s.to_datefrom employees_new e inner join salaries_new son e.emp_no = s.emp_no ; 结果为： 12345678910111213141516171819202122232425262728293031323334-- 1. 被驱动表没有索引，执行时间：大于800s，(800s未执行完)-- 执行计划：+----+-------------+-------+------------+------+---------------+------+---------+------+---------+----------+----------------------------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+---------+----------+----------------------------------------------------+| 1 | SIMPLE | e | NULL | ALL | NULL | NULL | NULL | NULL | 299389 | 100.00 | NULL || 1 | SIMPLE | s | NULL | ALL | NULL | NULL | NULL | NULL | 2837194 | 10.00 | Using where; Using join buffer (Block Nested Loop) |+----+-------------+-------+------------+------+---------------+------+---------+------+---------+----------+----------------------------------------------------+-- 2. 在被驱动表连接条件上创建索引，执行时间: 37s-- 创建索引语句create index idx_empno on salaries_new(emp_no);-- 执行计划：+----+-------------+-------+------------+------+---------------+-----------+---------+--------------------+--------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+-----------+---------+--------------------+--------+----------+-------+| 1 | SIMPLE | e | NULL | ALL | NULL | NULL | NULL | NULL | 299389 | 100.00 | NULL || 1 | SIMPLE | s | NULL | ref | idx_empno | idx_empno | 4 | employees.e.emp_no | 9 | 100.00 | NULL |+----+-------------+-------+------------+------+---------------+-----------+---------+--------------------+--------+----------+-------+-- 3. 更进一步，在驱动表连接条件上也创建索引，执行时间: 40s-- 创建索引语句create index idx_employees_new_empno on employees_new(emp_no);-- 执行计划：+----+-------------+-------+------------+------+-------------------------+-----------+---------+--------------------+--------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+-------------------------+-----------+---------+--------------------+--------+----------+-------+| 1 | SIMPLE | e | NULL | ALL | idx_employees_new_empno | NULL | NULL | NULL | 299389 | 100.00 | NULL || 1 | SIMPLE | s | NULL | ref | idx_empno | idx_empno | 4 | employees.e.emp_no | 9 | 100.00 | NULL |+----+-------------+-------+------------+------+-------------------------+-----------+---------+--------------------+--------+----------+-------+ 通过以上测试可见，在被驱动表的连接条件上创建索引是非常有必要的，而在驱动表连接条件上创建索引则不会显著提高速度。 例子2：对于inner join，考虑对连接条件和过滤条件(ON、WHERE)创建复合索引 进行表连接查询，语句如下(以下2个SQL在MySQL优化器中解析为相同SQL)： 12345678select e.emp_no,e.first_name,e.last_name,s.salary,s.from_date,s.to_datefrom employees_new e inner join salaries_new son e.emp_no = s.emp_no and e.first_name = &#x27;Georgi&#x27;-- 或者select e.emp_no,e.first_name,e.last_name,s.salary,s.from_date,s.to_datefrom employees_new e inner join salaries_new son e.emp_no = s.emp_nowhere e.first_name = &#x27;Georgi&#x27; 结果为： 123456789101112131415161718192021222324-- 1. 未在连接条件和过滤条件上创建复合索引，执行时间: 0.162s-- 执行计划：+----+-------------+-------+------------+------+-------------------------+-----------+---------+--------------------+--------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+-------------------------+-----------+---------+--------------------+--------+----------+-------------+| 1 | SIMPLE | e | NULL | ALL | idx_employees_new_empno | NULL | NULL | NULL | 299389 | 10.00 | Using where || 1 | SIMPLE | s | NULL | ref | idx_empno | idx_empno | 4 | employees.e.emp_no | 9 | 100.00 | NULL |+----+-------------+-------+------------+------+-------------------------+-----------+---------+--------------------+--------+----------+-------------+-- 2.在连接条件和过滤条件上创建复合索引，执行时间: 0.058s-- 创建索引语句create index idx_employees_first_name_emp_no on employees_new(first_name,emp_no);create index idx_employees_emp_no_first_name on employees_new(emp_no,first_name);-- 执行计划：+----+-------------+-------+------------+------+-----------------------------------------------------------------------------------------+---------------------------------+---------+--------------------+------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+-----------------------------------------------------------------------------------------+---------------------------------+---------+--------------------+------+----------+-------+| 1 | SIMPLE | e | NULL | ref | idx_employees_new_empno,idx_employees_first_name_emp_no,idx_employees_emp_no_first_name | idx_employees_first_name_emp_no | 16 | const | 253 | 100.00 | NULL || 1 | SIMPLE | s | NULL | ref | idx_empno | idx_empno | 4 | employees.e.emp_no | 9 | 100.00 | NULL |+----+-------------+-------+------------+------+-----------------------------------------------------------------------------------------+---------------------------------+---------+--------------------+------+----------+-------+ 通过以上测试可见，表的连接条件上和过滤条件上创建复合索引可以提高查询速度，从本例子看，速度没有较大提高，因为对employees_new表全表扫描速度很快，但是在非常大的表中，复合索引能够有效提高速度。 （5.2）inner join + 排序(group by 或者 order by)优化常规inner join+排序的SQL语法如下： 123SELECT &lt;select_list&gt;FROM &lt;left_table&gt; inner join &lt;right_table&gt; ON &lt;join_condition&gt;WHERE &lt;where_condition&gt;GROUP BY &lt;group_by_list&gt;ORDER BY &lt;order_by_list&gt; 优化方法： 1.与inner join一样，在被驱动表的连接条件上创建索引 2.inner join + 排序往往会在执行计划里面伴随着Using temporary Using filesort关键字出现，如果临时表或者排序的数据量很大，那么将会导致查询非常慢，需要特别重视；反之，临时表或者排序的数据量较小，例如只有几百条，那么即使执行计划有Using temporary Using filesort关键字，对查询速度影响也不大。如果说排序操作消耗了大部分的时间，那么可以考虑使用索引的有序性来消除排序，接下来对该优化方法进行讨论。 group by和order by都会对相关列进行排序，根据SQL是否存在GROUP BY或者ORDER BY关键字，分3种情况讨论： SQL语句存在****group by ****SQL语句存在****order by 优化操作考虑的排序列 解释 情况1 是 否 只需考虑group by相关列排序问题即可 如果SQL语句中只含有group by，则只需考虑group by后面的列排序问题即可 情况2 否 是 只需考虑order by相关列排序问题即可 如果SQL语句中只含有order by，则只需考虑order by后面的列排序问题即可 情况3 是 是 只需考虑group by相关列排序问题即可 如果SQL语句中同时含有group by和order by，只需考虑group by后面的排序即可。因为MySQL先执行group by，后执行order by，通常group by之后数据量已经较少了，后续的order by直接在磁盘上排序即可 对于上面3种情况： 1.如果优化考虑的排序列全部来源于驱动表，则可以考虑：在等值谓词过滤条件上+排序列上创建复合索引，这样可以使用索引先过滤数据，再使用索引按顺序获取数据。 2.如果优化考虑的排序列全部来源于某个被驱动表，则可以考虑：使用表连接hint(Straight_JOIN)控制连接顺序，将排序相关表设置为驱动表，然后按照1创建复合索引； 3.如果优化考虑的排序列来源于多个表，貌似没有好的解决办法，有想法的同学也可以留言，一起进步。 例子1：如果优化考虑的排序列全部来源于驱动表，则可以考虑：在等值谓词过滤条件上+排序列上创建复合索引，这样可以使用索引先过滤数据，再使用索引按顺序获取数据。 12345678910111213141516171819202122232425262728-- 1.驱动表e上存在排序mysql&gt; explain select e.first_name,sum(salary)from employees_new e inner join salaries_new s on e.emp_no &#x3D; s.emp_nowhere e.last_name &#x3D; &#39;Aamodt&#39;group by e.first_name;+----+-------------+-------+------------+------+------------------------------------------------------+------------------------------+---------+--------------------+------+----------+-----------------------------------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+------------------------------------------------------+------------------------------+---------+--------------------+------+----------+-----------------------------------------------------------+| 1 | SIMPLE | e | NULL | ref | idx_employees_new_empno,idx_lastname_empno_firstname | idx_lastname_empno_firstname | 18 | const | 205 | 100.00 | Using where; Using index; Using temporary; Using filesort || 1 | SIMPLE | s | NULL | ref | idx_empno | idx_empno | 4 | employees.e.emp_no | 9 | 100.00 | NULL |+----+-------------+-------+------------+------+------------------------------------------------------+------------------------------+---------+--------------------+------+----------+-----------------------------------------------------------+-- 2.在驱动表e上的等值谓词过滤条件last_name和排序列first_name上创建索引mysql&gt; create index idx_lastname_firstname on employees_new (last_name,first_name);-- 3.可以看到，排序消除mysql&gt; explain select e.first_name,sum(salary)from employees_new e inner join salaries_new s on e.emp_no &#x3D; s.emp_nowhere e.last_name &#x3D; &#39;Aamodt&#39;group by e.first_name;+----+-------------+-------+------------+------+----------------------------------------------------------------------------------+------------------------+---------+--------------------+------+----------+-----------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+----------------------------------------------------------------------------------+------------------------+---------+--------------------+------+----------+-----------------------+| 1 | SIMPLE | e | NULL | ref | idx_employees_new_empno,idx_employees_new_empno_firstname,idx_lastname_firstname | idx_lastname_firstname | 18 | const | 205 | 100.00 | Using index condition || 1 | SIMPLE | s | NULL | ref | idx_empno | idx_empno | 4 | employees.e.emp_no | 9 | 100.00 | NULL |+----+-------------+-------+------------+------+----------------------------------------------------------------------------------+------------------------+---------+--------------------+------+----------+-----------------------+ 需要说明的是，消除排序只是提供了一种数据优化的方式，消除排序后，其速度并不一定会比之前快，需要具体问题具体分析测试。 例子2：如果优化考虑的排序列全部来源于某个被驱动表，则可以考虑：使用表连接hint(Straight_JOIN)控制连接顺序，将排序相关表设置为驱动表，然后按照1创建复合索引； 12345678910111213141516171819202122232425262728293031323334353637383940414243444546-- 1. 被驱动表s上存在排序mysql&gt; explain select s.from_date,sum(salary)from employees_new e inner join salaries_new s on e.emp_no = s.emp_nowhere e.last_name = &#x27;Aamodt&#x27;and s.salary = 40000group by s.from_date;+----+-------------+-------+------------+------+------------------...-------+------------------------+---------+--------------------+------+----------+---------------------------------+| id | select_type | table | partitions | type | possible_keys ... | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+------------------...-------+------------------------+---------+--------------------+------+----------+---------------------------------+| 1 | SIMPLE | e | NULL | ref | idx_employees_new...stname | idx_lastname_firstname | 18 | const | 205 | 100.00 | Using temporary; Using filesort || 1 | SIMPLE | s | NULL | ref | idx_empno ... | idx_empno | 4 | employees.e.emp_no | 9 | 10.00 | Using where |+----+-------------+-------+------------+------+------------------...-------+------------------------+---------+--------------------+------+----------+---------------------------------+-- 2. 使用Straight_join改变表的连接顺序mysql&gt; explain select s.from_date,sum(salary)from salaries_new s STRAIGHT_JOIN employees_new e on e.emp_no = s.emp_nowhere e.last_name = &#x27;Aamodt&#x27;and s.salary = 40000group by s.from_date;+----+-------------+-------+------------+------+-----------------...----------+-------------------------+---------+--------------------+---------+----------+----------------------------------------------+| id | select_type | table | partitions | type | possible_keys ... | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+-----------------...----------+-------------------------+---------+--------------------+---------+----------+----------------------------------------------+| 1 | SIMPLE | s | NULL | ALL | idx_empno ... | NULL | NULL | NULL | 2837194 | 10.00 | Using where; Using temporary; Using filesort || 1 | SIMPLE | e | NULL | ref | idx_employees_ne...firstname | idx_employees_new_empno | 4 | employees.s.emp_no | 1 | 5.00 | Using where |+----+-------------+-------+------------+------+-----------------...----------+-------------------------+---------+--------------------+---------+----------+----------------------------------------------+-- 3. 在新的驱动表上创建等值谓词+排序列索引mysql&gt; create index idx_salary_fromdate on salaries_new(salary,from_date);Query OK, 0 rows affected (5.39 sec)Records: 0 Duplicates: 0 Warnings: 0-- 4. 可以看到，消除排序mysql&gt; explain select s.from_date,sum(salary)from salaries_new s STRAIGHT_JOIN employees_new e on e.emp_no = s.emp_nowhere e.last_name = &#x27;Aamodt&#x27;and s.salary = 40000group by s.from_date;+----+-------------+-------+------------+------+---------------------------------...--+-------------------------+---------+--------------------+--------+----------+-----------------------+| id | select_type | table | partitions | type | possible_keys ... | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------------------------...--+-------------------------+---------+--------------------+--------+----------+-----------------------+| 1 | SIMPLE | s | NULL | ref | idx_empno,idx_salary_fromdate ... | idx_salary_fromdate | 4 | const | 199618 | 100.00 | Using index condition || 1 | SIMPLE | e | NULL | ref | idx_employees_new_empno,idx_empl...e | idx_employees_new_empno | 4 | employees.s.emp_no | 1 | 5.00 | Using where |+----+-------------+-------+------------+------+---------------------------------...--+-------------------------+---------+--------------------+--------+----------+-----------------------+ 需要说明的是，大部分情况下，MySQL优化器会自动选择最优的表连接方式，Straight_join的引入往往会造成大表做驱动表的情况出现，虽然消除了排序，但是又引入了新的麻烦。到底是排序带来的开销大，还是NLJ循环嵌套不合理带来的开销大，需要具体情况具体分析。 （5.3）left join优化在MySQL中外连接(left join、right join 、full join)会被优化器转换为left join，因此，外连接只需讨论left join即可。常规left join的SQL语法如下： 12345SELECT &lt;select_list&gt;FROM &lt;left_table&gt; left join &lt;right_table&gt; ON &lt;join_condition&gt;WHERE &lt;where_condition&gt;GROUP BY &lt;group_by_list&gt;ORDER BY &lt;order_by_list&gt; 优化方法： 1.与inner join一样，在被驱动表的连接条件上创建索引 2.left join的表连接顺序都是从左像右的，我们无法改变表连接顺序。但是如果右表在where条件中存在谓词过滤，则MySQL会将left join自动转换为inner join，其原理图如下： 例子1：.如果右表在where条件中存在谓词过滤，则MySQL会将left join自动转换为inner join 创建测试表： 123456789101112131415create table dept( deptno int, dname varchar(20));insert into dept values (10, &#x27;sales&#x27;),(20, &#x27;hr&#x27;),(30, &#x27;product&#x27;),(40, &#x27;develop&#x27;);create table emp ( empno int, ename varchar(20), deptno varchar(20));insert into emp values (1,&#x27;aa&#x27;,10),(2,&#x27;bb&#x27;,10),(3,&#x27;cc&#x27;,20),(4,&#x27;dd&#x27;,30),(5,&#x27;ee&#x27;,30); 执行left join，查看其执行计划，发现并不是左表作为驱动表 12345678910mysql&gt; explain select d.dname,e.enamefrom dept d left join emp eon d.deptno = e.deptnowhere e.deptno = 30;+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+----------------------------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+----------------------------------------------------+| 1 | SIMPLE | e | NULL | ALL | NULL | NULL | NULL | NULL | 5 | 20.00 | Using where || 1 | SIMPLE | d | NULL | ALL | NULL | NULL | NULL | NULL | 4 | 25.00 | Using where; Using join buffer (Block Nested Loop) |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+----------------------------------------------------+ 通过trace追踪，发现MySQL对其该语句进行了等价改写，将外连接改为了内连接。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244mysql&gt; set optimizer_trace=&quot;enabled=on&quot;,end_markers_in_JSON=on;Query OK, 0 rows affected (0.00 sec)mysql&gt; select d.dname,e.enamefrom dept d left join emp eon d.deptno = e.deptnowhere e.deptno = 30;+---------+-------+| dname | ename |+---------+-------+| product | dd || product | ee |+---------+-------+2 rows in set (0.03 sec)mysql&gt; select * from information_schema.optimizer_trace;| MISSING_BYTES_BEYOND_MAX_MEM_SIZE | INSUFFICIENT_PRIVILEGES |select d.dname,e.enamefrom dept d left join emp eon d.deptno = e.deptnowhere e.deptno = 30 | &#123; &quot;steps&quot;: [ &#123; &quot;join_preparation&quot;: &#123; &quot;select#&quot;: 1, &quot;steps&quot;: [ &#123; &quot;expanded_query&quot;: &quot;/* select#1 */ select `d`.`dname` AS `dname`,`e`.`ename` AS `ename` from (`dept` `d` left join `emp` `e` on((`d`.`deptno` = `e`.`deptno`))) where (`e`.`deptno` = 30)&quot; &#125;, &#123; &quot;transformations_to_nested_joins&quot;: &#123; &quot;transformations&quot;: [ &quot;outer_join_to_inner_join&quot;, &quot;JOIN_condition_to_WHERE&quot;, &quot;parenthesis_removal&quot; ] /* transformations */, &quot;expanded_query&quot;: &quot;/* select#1 */ select `d`.`dname` AS `dname`,`e`.`ename` AS `ename` from `dept` `d` join `emp` `e` where ((`e`.`deptno` = 30) and (`d`.`deptno` = `e`.`deptno`))&quot; &#125; /* transformations_to_nested_joins */ &#125; ] /* steps */ &#125; /* join_preparation */ &#125;, &#123; &quot;join_optimization&quot;: &#123; &quot;select#&quot;: 1, &quot;steps&quot;: [ &#123; &quot;condition_processing&quot;: &#123; &quot;condition&quot;: &quot;WHERE&quot;, &quot;original_condition&quot;: &quot;((`e`.`deptno` = 30) and (`d`.`deptno` = `e`.`deptno`))&quot;, &quot;steps&quot;: [ &#123; &quot;transformation&quot;: &quot;equality_propagation&quot;, &quot;resulting_condition&quot;: &quot;((`e`.`deptno` = 30) and (`d`.`deptno` = `e`.`deptno`))&quot; &#125;, &#123; &quot;transformation&quot;: &quot;constant_propagation&quot;, &quot;resulting_condition&quot;: &quot;((`e`.`deptno` = 30) and (`d`.`deptno` = `e`.`deptno`))&quot; &#125;, &#123; &quot;transformation&quot;: &quot;trivial_condition_removal&quot;, &quot;resulting_condition&quot;: &quot;((`e`.`deptno` = 30) and (`d`.`deptno` = `e`.`deptno`))&quot; &#125; ] /* steps */ &#125; /* condition_processing */ &#125;, &#123; &quot;substitute_generated_columns&quot;: &#123; &#125; /* substitute_generated_columns */ &#125;, &#123; &quot;table_dependencies&quot;: [ &#123; &quot;table&quot;: &quot;`dept` `d`&quot;, &quot;row_may_be_null&quot;: false, &quot;map_bit&quot;: 0, &quot;depends_on_map_bits&quot;: [ ] /* depends_on_map_bits */ &#125;, &#123; &quot;table&quot;: &quot;`emp` `e`&quot;, &quot;row_may_be_null&quot;: true, &quot;map_bit&quot;: 1, &quot;depends_on_map_bits&quot;: [ ] /* depends_on_map_bits */ &#125; ] /* table_dependencies */ &#125;, &#123; &quot;ref_optimizer_key_uses&quot;: [ ] /* ref_optimizer_key_uses */ &#125;, &#123; &quot;rows_estimation&quot;: [ &#123; &quot;table&quot;: &quot;`dept` `d`&quot;, &quot;table_scan&quot;: &#123; &quot;rows&quot;: 4, &quot;cost&quot;: 1 &#125; /* table_scan */ &#125;, &#123; &quot;table&quot;: &quot;`emp` `e`&quot;, &quot;table_scan&quot;: &#123; &quot;rows&quot;: 5, &quot;cost&quot;: 1 &#125; /* table_scan */ &#125; ] /* rows_estimation */ &#125;, &#123; &quot;considered_execution_plans&quot;: [ &#123; &quot;plan_prefix&quot;: [ ] /* plan_prefix */, &quot;table&quot;: &quot;`dept` `d`&quot;, &quot;best_access_path&quot;: &#123; &quot;considered_access_paths&quot;: [ &#123; &quot;rows_to_scan&quot;: 4, &quot;access_type&quot;: &quot;scan&quot;, &quot;resulting_rows&quot;: 4, &quot;cost&quot;: 1.8, &quot;chosen&quot;: true &#125; ] /* considered_access_paths */ &#125; /* best_access_path */, &quot;condition_filtering_pct&quot;: 100, &quot;rows_for_plan&quot;: 4, &quot;cost_for_plan&quot;: 1.8, &quot;rest_of_plan&quot;: [ &#123; &quot;plan_prefix&quot;: [ &quot;`dept` `d`&quot; ] /* plan_prefix */, &quot;table&quot;: &quot;`emp` `e`&quot;, &quot;best_access_path&quot;: &#123; &quot;considered_access_paths&quot;: [ &#123; &quot;rows_to_scan&quot;: 5, &quot;access_type&quot;: &quot;scan&quot;, &quot;using_join_cache&quot;: true, &quot;buffers_needed&quot;: 1, &quot;resulting_rows&quot;: 1, &quot;cost&quot;: 2.6007, &quot;chosen&quot;: true &#125; ] /* considered_access_paths */ &#125; /* best_access_path */, &quot;condition_filtering_pct&quot;: 100, &quot;rows_for_plan&quot;: 4, &quot;cost_for_plan&quot;: 4.4007, &quot;chosen&quot;: true &#125; ] /* rest_of_plan */ &#125;, &#123; &quot;plan_prefix&quot;: [ ] /* plan_prefix */, &quot;table&quot;: &quot;`emp` `e`&quot;, &quot;best_access_path&quot;: &#123; &quot;considered_access_paths&quot;: [ &#123; &quot;rows_to_scan&quot;: 5, &quot;access_type&quot;: &quot;scan&quot;, &quot;resulting_rows&quot;: 1, &quot;cost&quot;: 2, &quot;chosen&quot;: true &#125; ] /* considered_access_paths */ &#125; /* best_access_path */, &quot;condition_filtering_pct&quot;: 100, &quot;rows_for_plan&quot;: 1, &quot;cost_for_plan&quot;: 2, &quot;rest_of_plan&quot;: [ &#123; &quot;plan_prefix&quot;: [ &quot;`emp` `e`&quot; ] /* plan_prefix */, &quot;table&quot;: &quot;`dept` `d`&quot;, &quot;best_access_path&quot;: &#123; &quot;considered_access_paths&quot;: [ &#123; &quot;rows_to_scan&quot;: 4, &quot;access_type&quot;: &quot;scan&quot;, &quot;using_join_cache&quot;: true, &quot;buffers_needed&quot;: 1, &quot;resulting_rows&quot;: 4, &quot;cost&quot;: 1.8002, &quot;chosen&quot;: true &#125; ] /* considered_access_paths */ &#125; /* best_access_path */, &quot;condition_filtering_pct&quot;: 100, &quot;rows_for_plan&quot;: 4, &quot;cost_for_plan&quot;: 3.8002, &quot;chosen&quot;: true &#125; ] /* rest_of_plan */ &#125; ] /* considered_execution_plans */ &#125;, &#123; &quot;attaching_conditions_to_tables&quot;: &#123; &quot;original_condition&quot;: &quot;((`e`.`deptno` = 30) and (`d`.`deptno` = `e`.`deptno`))&quot;, &quot;attached_conditions_computation&quot;: [ ] /* attached_conditions_computation */, &quot;attached_conditions_summary&quot;: [ &#123; &quot;table&quot;: &quot;`emp` `e`&quot;, &quot;attached&quot;: &quot;(`e`.`deptno` = 30)&quot; &#125;, &#123; &quot;table&quot;: &quot;`dept` `d`&quot;, &quot;attached&quot;: &quot;(`d`.`deptno` = `e`.`deptno`)&quot; &#125; ] /* attached_conditions_summary */ &#125; /* attaching_conditions_to_tables */ &#125;, &#123; &quot;refine_plan&quot;: [ &#123; &quot;table&quot;: &quot;`emp` `e`&quot; &#125;, &#123; &quot;table&quot;: &quot;`dept` `d`&quot; &#125; ] /* refine_plan */ &#125; ] /* steps */ &#125; /* join_optimization */ &#125;, &#123; &quot;join_execution&quot;: &#123; &quot;select#&quot;: 1, &quot;steps&quot;: [ ] /* steps */ &#125; /* join_execution */ &#125; ] /* steps */&#125; | 0 | 0 |+----------------------------------------------------------------------------mysql&gt;","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"}]},{"title":"Oracle spatial相关","slug":"Oracle spatial相关","date":"2021-05-26T08:59:03.000Z","updated":"2023-06-04T13:14:15.190Z","comments":true,"path":"2021/05/26/Oracle spatial相关/","link":"","permalink":"http://example.com/2021/05/26/Oracle%20spatial%E7%9B%B8%E5%85%B3/","excerpt":"","text":"Oracle Spatial中SDO_GEOMETRY类型： 1234567CREATE TYPE SDO_GEOMETRY AS OBJECT( SDO_GTYPE NUMBER,--几何类型，如点线面SDO_SRID NUMBER,--几何的空间参考坐标系SDO_POINT SDO_POINT_TYPE,--如果几何为点类型，则存储点坐标，否则为空SDO_ELEM_INFO SDO_ELEM_INFO_ARRAY,--定义如何理解SDO_ORDINATES中的坐标序列SDO_ORDINATES SDO_ORDINATE_ARRAY--存储实际坐标，以X,Y以及不同点之间以逗号隔开) 两个对象之间关系RELATESDO_GEOM.RELATE –确定两个对象的交互方式 12SDO_GEOM.RELATE(geom1 IN SDO_GEOMETRY,dim1 IN SDO_DIM_ARRAY,mask IN VARCHAR2, geom2 IN SDO_GEOMETRY,dim2 IN SDO_DIM_ARRAY) RETURN VARCHAR2;SDO_GEOM.RELATE(geom1 IN SDO_GEOMETRY,mask IN VARCHAR2,geom2 IN SDO_GEOMETRY, tol IN NUMBER)RETURN VARCHAR2;--tol容许的精度范围 mask: ANYINTERACT:geom2落在geom1面上，包括边上 CONTAINS:geom2完全包含在geom1几何对象中，并且两个几何对象的边没有交叉 COVEREDBY:geom1完全包含在geom2中，并且两个几何对象的边有一个或多个点相互重叠 COVERS:geom2完全包含在geom1中，并且两个几何对象的边有一个或多个点相互重叠 DISJOINT:两个几何没有重叠交叉点，也没有共同的边 EQUAL:两个几何相等 INSIDE:geom1完全包含在geom2几何对象中，并且两个几何对象的边没有交叉 ON:geom1的边和内部的线完全在geom2上 OVERLAPBDYDISJOINT:两个几何对象交迭，但是边没有交叉 OVERLAPBDYINTERSECT:两个几何对象叫迭，并且边有部分交叉 TOUCH:两个结合对象有共同的边，但没有交叉 WITHIN_DISTANCESDO_GEOM.WITHIN_DISTANCE –确定两个几何是否在彼此指定的距离之内 12SDO_GEOM.WITHIN_DISTANCE(geom1 IN SDO_GEOMETRY,dim1 IN SDO_DIM_ARRAY,dist IN NUMBER, geom2 IN SDO_GEOMETRY,dim2 IN SDO_DIM_ARRAY[, units IN VARCHAR2]) RETURN VARCHAR2;SDO_GEOM.WITHIN_DISTANCE(geom1 IN SDO_GEOMETRY,dist IN NUMBER,geom2 IN SDO_GEOMETRY,tol IN NUMBER[, units IN VARCHAR2]) RETURN VARCHAR2; 验证：VALIDATE_GEOMETRY_WITH_CONTEXTSDO_GEOM.VALIDATE_GEOMETRY_WITH_CONTEXT –确定几何是否有效，如果几何无效，则返回上下文信息 12SDO_GEOM.VALIDATE_GEOMETRY_WITH_CONTEXT(theGeometry IN SDO_GEOMETRY, theDimInfo IN SDO_DIM_ARRAY,conditional IN VARCHAR2 DEFAULT &#x27;TRUE&#x27;) RETURN VARCHAR2;SDO_GEOM.VALIDATE_GEOMETRY_WITH_CONTEXT(theGeometry IN SDO_GEOMETRY,tolerance IN NUMBER,conditional IN VARCHAR2 DEFAULT &#x27;TRUE&#x27;) RETURN VARCHAR2; VALIDATE_LAYER_WITH_CONTEXTSDO_GEOM.VALIDATE_LAYER_WITH_CONTEXT –确定存储在列中的所有几何是否有效，并返回有关任何无效几何的上下文信息 1SDO_GEOM.VALIDATE_LAYER_WITH_CONTEXT(geom_table IN VARCHAR2,geom_column IN VARCHAR2,result_table IN VA 单对象操作：SDO_ARC_DENSIFYSDO_GEOM.SDO_ARC_DENSIFY –简化，将圆弧简化成由直线组成的近似多段线；将圆近似成N多边形 eg: 12SDO_GEOM.SDO_ARC_DENSIFY(geom IN SDO_GEOMETRY,dim IN SDO_DIM_ARRAY, params IN VARCHAR2) RETURN SDO_GEOMETRY;SDO_GEOM.SDO_ARC_DENSIFY(geom IN SDO_GEOMETRY,tol IN NUMBER, params IN VARCHAR2) RETURN SDO_GEOMETRY; SDO_AREASDO_GEOM.SDO_AREA –计算多边形的面积 eg: 12SDO_GEOM.SDO_AREA(geom IN SDO_GEOMETRY,dim IN SDO_DIM_ARRAY [, unit IN VARCHAR2]) RETURN NUMBER;SDO_GEOM.SDO_AREA(geom IN SDO_GEOMETRY,tol IN NUMBER [, unit IN VARCHAR2]) RETURN NUMBER; SDO_BUFFERSDO_GEOM.SDO_BUFFER –在几何体外部或内部生成缓冲区 eg: 12SDO_GEOM.SDO_BUFFER(geom IN SDO_GEOMETRY,dim IN SDO_DIM_ARRAY,dist IN NUMBER[, params IN VARCHAR2]) RETURN SDO_GEOMETRY;SDO_GEOM.SDO_BUFFER(geom IN SDO_GEOMETRY,dist IN NUMBER,tol IN NUMBER[, params IN VARCHAR2]) RETURN SDO_GEOMETRY; SDO_CENTROIDSDO_GEOM.SDO_CENTROID –计算多边形的质心 eg: 12SDO_GEOM.SDO_CENTROID(geom1 IN SDO_GEOMETRY,dim1 IN SDO_DIM_ARRAY) RETURN SDO_GEOMETRY;SDO_GEOM.SDO_CENTROID(geom1 IN SDO_GEOMETRY,tol IN NUMBER) RETURN SDO_GEOMETRY; SDO_CONVEXHULLSDO_GEOM.SDO_CONVEXHULL –返回几何对象的凸包的多边形类型对象 eg: 12SDO_GEOM.SDO_CONVEXHULL(geom1 IN SDO_GEOMETRY,dim1 IN SDO_DIM_ARRAY) RETURN SDO_GEOMETRY;SDO_GEOM.SDO_CONVEXHULL(geom1 IN SDO_GEOMETRY, tol IN NUMBER) RETURN SDO_GEOMETRY; SDO_LENGTHSDO_GEOM.SDO_LENGTH –计算几何的长度或周长 eg: 12SDO_GEOM.SDO_LENGTH(geom IN SDO_GEOMETRY,dim IN SDO_DIM_ARRAY[, unit IN VARCHAR2][, count_shared_edges IN NUMBER]) RETURN NUMBER;SDO_GEOM.SDO_LENGTH(geom IN SDO_GEOMETRY,tol IN NUMBER[, unit IN VARCHAR2] [, count_shared_edges IN NUMBER]) RETURN NUMBER; SDO_MAX_MBR_ORDINATESDO_GEOM.SDO_MAX_MBR_ORDINATE –返回几何对象最小边界矩形的指定纵坐标的最大值 eg: 12SDO_GEOM.SDO_MAX_MBR_ORDINATE(geom IN SDO_GEOMETRY,ordinate_pos IN NUMBER) RETURN NUMBER;SDO_GEOM.SDO_MAX_MBR_ORDINATE(geom IN SDO_GEOMETRY,dim IN SDO_DIM_ARRAY,ordinate_pos IN NUMBER) RETURN NUMBER; SDO_MIN_MBR_ORDINATESDO_GEOM.SDO_MIN_MBR_ORDINATE –返回几何对象最小边界矩形的指定纵坐标的最小值 eg: 12SDO_GEOM.SDO_MIN_MBR_ORDINATE(geom IN SDO_GEOMETRY,ordinate_pos IN NUMBER ) RETURN NUMBER;SDO_GEOM.SDO_MIN_MBR_ORDINATE(geom IN SDO_GEOMETRY,dim IN SDO_DIM_ARRAY,ordinate_pos IN NUMBER) RETURN NUMBER; SDO_MBRSDO_GEOM.SDO_MBR –返回几何对象最小边界矩形 eg: 1SDO_GEOM.SDO_MBR(geom IN SDO_GEOMETRY[, dim IN SDO_DIM_ARRAY]) RETURN SDO_GEOMETRY; SDO_POINTONSURFACESDO_GEOM.SDO_POINTONSURFACE –返回一个保证在多边形表面上的点 eg: 12SDO_GEOM.SDO_POINTONSURFACE(geom1 IN SDO_GEOMETRY,dim1 IN SDO_DIM_ARRAY) RETURN SDO_GEOMETRY;SDO_GEOM.SDO_POINTONSURFACE(geom1 IN SDO_GEOMETRY,tol IN NUMBER) RETURN SDO_GEOMETRY; SDO_VOLUMESDO_GEOM.SDO_VOLUME –计算三维立体几何体的体积 eg: 1SDO_GEOM.SDO_VOLUME(geom IN SDO_GEOMETRY,tol IN NUMBER[, unit IN VARCHAR2]) RETURN NUMBER; 双目标操作：SDO_CLOSEST_POINTSSDO_GEOM.SDO_CLOSEST_POINTS –计算两个几何之间的最小距离，并返回距离最小时在几何上的两点 eg: 1SDO_GEOM.SDO_CLOSEST_POINTS(geom1 IN SDO_GEOMETRY,geom2 IN SDO_GEOMETRY,tolerance IN NUMBER,unit IN VARCHAR2,dist SDO_DISTANCESDO_GEOM.SDO_DISTANCE –计算两个几何对象之间的距离 eg: 12SDO_GEOM.SDO_DISTANCE(geom1 IN SDO_GEOMETRY,dim1 IN SDO_DIM_ARRAY,geom2 IN SDO_GEOMETRY,dim2 IN SDO_DIM_ARRAY[, unit IN VARCHAR2]) RETURN NUMBER;SDO_GEOM.SDO_DISTANCE(geom1 IN SDO_GEOMETRY,geom2 IN SDO_GEOMETRY,tol IN NUMBER [, unit IN VARCHAR2]) RETURN NUMBER; SDO_DIFFERENCESDO_GEOM.SDO_DIFFERENCE –返回几何对象（两个几何对象的拓扑差异，MINUS操作） eg: 12SDO_GEOM.SDO_DIFFERENCE(geom1 IN SDO_GEOMETRY,dim1 IN SDO_DIM_ARRAY,geom2 IN SDO_GEOMETRY,dim2 IN SDO_DIM_ARRAY) RETURN SDO_GEOMETRY;SDO_GEOM.SDO_DIFFERENCE(geom1 IN SDO_GEOMETRY,geom2 IN SDO_GEOMETRY, tol IN NUMBER) RETURN SDO_GEOMETRY; SDO_INTERSECTIONSDO_GEOM.SDO_INTERSECTION –返回几何对象（两个几何对象的拓扑交点，AND操作） eg: 12SDO_GEOM.SDO_INTERSECTION(geom1 IN SDO_GEOMETRY,dim1 IN SDO_DIM_ARRAY,geom2 IN SDO_GEOMETRY,dim2 IN SDO_DIM_ARRAY) RETURN SDO_GEOMETRY;SDO_GEOM.SDO_INTERSECTION(geom1 IN SDO_GEOMETRY,geom2 IN SDO_GEOMETRY, tol IN NUMBER) RETURN SDO_GEOMETRY; SDO_UNIONSDO_GEOM.SDO_UNION –返回几何对象（两个几何对象的拓扑结合，OR操作） eg: 12SDO_GEOM.SDO_UNION(geom1 IN SDO_GEOMETRY,dim1 IN SDO_DIM_ARRAY, geom2 IN SDO_GEOMETRY,dim2 IN SDO_DIM_ARRAY) RETURN SDO_GEOMETRY;SDO_GEOM.SDO_UNION(geom1 IN SDO_GEOMETRY,geom2 IN SDO_GEOMETRY, tol IN NUMBER ) RETURN SDO_GEOMETRY; SDO_XORSDO_GEOM.SDO_XOR –返回几何对象（两个几何对象的拓扑对称差异，XOR操作） eg: 12SDO_GEOM.SDO_XOR(geom1 IN SDO_XOR,dim1 IN SDO_DIM_ARRAY,geom2 IN SDO_GEOMETRY,dim2 IN SDO_DIM_ARRAY) RETURN SDO_GEOMETRY;SDO_GEOM.SDO_XOR(geom1 IN SDO_GEOMETRY,geom2 IN SDO_GEOMETRY, tol IN NUMBER ) RETURN SDO_GEOMETRY; 实例：12345678910--wkt转geometryselect sdo_geometry(TO_CHAR(SDO_GEOMETRY(2003, NULL, NULL, SDO_ELEM_INFO_ARRAY(1, 1003, 1), SDO_ORDINATE_ARRAY(5, 5, 5, 7, 1, 7, 1, 1, 5, 1, 5, 3, 6, 3, 6, 5, 5, 5)).get_wkt())) from dual--wkt转geometryselect SDO_UTIL.from_wktgeometry(TO_CHAR(SDO_GEOMETRY(2003, NULL, NULL, SDO_ELEM_INFO_ARRAY(1, 1003, 1), SDO_ORDINATE_ARRAY(5, 5, 5, 7, 1, 7, 1, 1, 5, 1, 5, 3, 6, 3, 6, 5, 5, 5)).get_wkt())) from dual--geometry转wktselect TO_CHAR(SDO_GEOMETRY(2003, NULL, NULL, SDO_ELEM_INFO_ARRAY(1, 1003, 1), SDO_ORDINATE_ARRAY(5, 5, 5, 7, 1, 7, 1, 1, 5, 1, 5, 3, 6, 3, 6, 5, 5, 5)).get_wkt()) from dual--geometry转wktselect SDO_UTIL.to_wktgeometry_varchar(SDO_GEOMETRY(2003, NULL, NULL, SDO_ELEM_INFO_ARRAY(1, 1003, 1), SDO_ORDINATE_ARRAY(5, 5, 5, 7, 1, 7, 1, 1, 5, 1, 5, 3, 6, 3, 6, 5, 5, 5))) from dual--geometry转wkt（不受oracle字段只能少于4000个字符长度影响）select SDO_UTIL.to_wktgeometry(SDO_GEOMETRY(2003, NULL, NULL, SDO_ELEM_INFO_ARRAY(1, 1003, 1), SDO_ORDINATE_ARRAY(5, 5, 5, 7, 1, 7, 1, 1, 5, 1, 5, 3, 6, 3, 6, 5, 5, 5))) from dual 12345678--合并select SDO_GEOM.SDO_UNION(SDO_GEOMETRY(&#x27;POLYGON((1 1,5 1,5 5,1 5,1 1))&#x27;),SDO_GEOMETRY(&#x27;POLYGON((2 2,2 3,3 3,3 2,2 2))&#x27;),0.001 ) from dual;--缓冲区select SDO_UTIL.to_wktgeometry_varchar(SDO_GEOM.SDO_BUFFER(SDO_GEOMETRY(&#x27;POLYGON((1 1,5 1,5 5,1 5,1 1))&#x27;),1000,0.01)) from dual;--点在面内select SDO_GEOM.RELATE(SDO_GEOMETRY(&#x27;POLYGON((1 1,5 1,5 5,1 5,1 1))&#x27;),&#x27;CONTAINS&#x27;,SDO_GEOMETRY(&#x27;POINT(12 2)&#x27;), 0.01) from dual;--对象相交 SDO_GEOMETRY(&#x27;POLYGON((0 0,0 1,1 1,1 0,0 0))&#x27;, srid)select SDO_GEOM.RELATE(SDO_GEOMETRY(&#x27;POLYGON((0 0,0 1,1 1,1 0,0 0))&#x27;),&#x27;ANYINTERACT&#x27;,SDO_GEOMETRY(&#x27;POLYGON((0.5 0.5,0.5 1.5,1.5 1.5,1.5 0.5,0.5 0.5))&#x27;), 0.01) from dual; 参考文章（其它）https://www.cnblogs.com/lanzi/archive/2010/12/28/1918803.html官方：https://datacadamia.com/oracle_spatial/geometry","categories":[],"tags":[{"name":"oracle","slug":"oracle","permalink":"http://example.com/tags/oracle/"},{"name":"gis","slug":"gis","permalink":"http://example.com/tags/gis/"},{"name":"spatial","slug":"spatial","permalink":"http://example.com/tags/spatial/"}]},{"title":"HashTable源码解析","slug":"HashTable源码解析","date":"2021-05-25T13:12:30.000Z","updated":"2023-06-04T13:16:32.143Z","comments":true,"path":"2021/05/25/HashTable源码解析/","link":"","permalink":"http://example.com/2021/05/25/HashTable%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/","excerpt":"","text":"HashTable是Map接口线程安全实现版本，数据结构和方法实现与HashMap类似，本文记录HashTable源码解析，基于JDK1.8。 类结构HashTable类层级关系图： 主要成员变量： 12345678910// 内部采用Entry数组存储键值对数据，Entry实际为单向链表的表头private transient Entry&lt;?,?&gt;[] table;// HashTable里键值对个数private transient int count;// 扩容阈值，当超过这个值时，进行扩容操作，计算方式为：数组容量*加载因子private int threshold;// 加载因子private float loadFactor;// 用于快速失败private transient int modCount = 0; table属性通过transient修饰，原因在介绍HashMap源码的时候分析过。 Entry代码如下： 123456789101112131415private static class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; V value; Entry&lt;K,V&gt; next; protected Entry(int hash, K key, V value, Entry&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; ......&#125; Entry为单向链表节点，HashTable采用数组加链表的方式存储数据，不过没有类似于HashMap中当链表过长时转换为红黑树的操作。 方法解析构造函数1234567891011121314151617181920212223242526// 设置指定容量和加载因子，初始化HashTablepublic Hashtable(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(&quot;Illegal Capacity: &quot;+ initialCapacity); if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(&quot;Illegal Load: &quot;+loadFactor); if (initialCapacity==0) // 容量最小为1 initialCapacity = 1; this.loadFactor = loadFactor; table = new Entry&lt;?,?&gt;[initialCapacity]; // 初始扩容阈值 threshold = (int)Math.min(initialCapacity * loadFactor, MAX_ARRAY_SIZE + 1);&#125;// 设置指定容量初始HashTable，加载因子为0.75public Hashtable(int initialCapacity) &#123; this(initialCapacity, 0.75f);&#125;// 手动指定数组初始容量为11，加载因子为0.75public Hashtable() &#123; this(11, 0.75f);&#125; put(K key, V value)put(K key, V value)添加指定键值对，键和值都不能为null： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455// 方法synchronized修饰，线程安全public synchronized V put(K key, V value) &#123; // Make sure the value is not null if (value == null) &#123; throw new NullPointerException(); &#125; // Makes sure the key is not already in the hashtable. Entry&lt;?,?&gt; tab[] = table; // 得到key的哈希值 int hash = key.hashCode(); // 得到该key存在到数组中的下标 int index = (hash &amp; 0x7FFFFFFF) % tab.length; @SuppressWarnings(&quot;unchecked&quot;) // 得到该下标对应的Entry Entry&lt;K,V&gt; entry = (Entry&lt;K,V&gt;)tab[index]; // 如果该下标的Entry不为null，则进行链表遍历 for(; entry != null ; entry = entry.next) &#123; // 遍历链表，如果存在key相等的节点，则替换这个节点的值，并返回旧值 if ((entry.hash == hash) &amp;&amp; entry.key.equals(key)) &#123; V old = entry.value; entry.value = value; return old; &#125; &#125; // 如果数组下标对应的节点为空，或者遍历链表后发现没有和该key相等的节点，则执行插入操作 addEntry(hash, key, value, index); return null;&#125;private void addEntry(int hash, K key, V value, int index) &#123; // 模数+1 modCount++; Entry&lt;?,?&gt; tab[] = table; // 判断是否需要扩容 if (count &gt;= threshold) &#123; // 如果count大于等于扩容阈值，则进行扩容 rehash(); tab = table; // 扩容后，重新计算该key在扩容后table里的下标 hash = key.hashCode(); index = (hash &amp; 0x7FFFFFFF) % tab.length; &#125; // Creates the new entry. @SuppressWarnings(&quot;unchecked&quot;) // 采用头插的方式插入，index位置的节点为新节点的next节点 // 新节点取代inde位置节点 Entry&lt;K,V&gt; e = (Entry&lt;K,V&gt;) tab[index]; tab[index] = new Entry&lt;&gt;(hash, key, value, e); // count+1 count++;&#125; rehash()rehash扩容操作： 123456789101112131415161718192021222324252627282930313233343536protected void rehash() &#123; // 暂存旧的table和容量 int oldCapacity = table.length; Entry&lt;?,?&gt;[] oldMap = table; // 新容量为旧容量的2n+1倍 int newCapacity = (oldCapacity &lt;&lt; 1) + 1; // 判断新容量是否超过最大容量 if (newCapacity - MAX_ARRAY_SIZE &gt; 0) &#123; // 如果旧容量已经是最大容量大话，就不扩容了 if (oldCapacity == MAX_ARRAY_SIZE) // Keep running with MAX_ARRAY_SIZE buckets return; // 新容量最大值只能是MAX_ARRAY_SIZE newCapacity = MAX_ARRAY_SIZE; &#125; // 用新容量创建一个新Entry数组 Entry&lt;?,?&gt;[] newMap = new Entry&lt;?,?&gt;[newCapacity]; // 模数+1 modCount++; // 重新计算下次扩容阈值 threshold = (int)Math.min(newCapacity * loadFactor, MAX_ARRAY_SIZE + 1); // 将新Entry数组赋值给table table = newMap; // 遍历数组和链表，进行新table赋值操作 for (int i = oldCapacity ; i-- &gt; 0 ;) &#123; for (Entry&lt;K,V&gt; old = (Entry&lt;K,V&gt;)oldMap[i] ; old != null ; ) &#123; Entry&lt;K,V&gt; e = old; old = old.next; int index = (e.hash &amp; 0x7FFFFFFF) % newCapacity; e.next = (Entry&lt;K,V&gt;)newMap[index]; newMap[index] = e; &#125; &#125;&#125; get(Object key)get(Object key)获取指定key对应的value： 123456789101112public synchronized V get(Object key) &#123; Entry&lt;?,?&gt; tab[] = table; int hash = key.hashCode(); // 根据key哈希得到index，遍历链表取值 int index = (hash &amp; 0x7FFFFFFF) % tab.length; for (Entry&lt;?,?&gt; e = tab[index] ; e != null ; e = e.next) &#123; if ((e.hash == hash) &amp;&amp; e.key.equals(key)) &#123; return (V)e.value; &#125; &#125; return null;&#125; synchronized修饰，线程安全。 remove(Object key)remove(Object key)删除指定key，返回对应的value： 123456789101112131415161718192021222324public synchronized V remove(Object key) &#123; Entry&lt;?,?&gt; tab[] = table; int hash = key.hashCode(); // 获取key对应的index int index = (hash &amp; 0x7FFFFFFF) % tab.length; @SuppressWarnings(&quot;unchecked&quot;) // 遍历链表，如果找到key相等的节点，则改变前继和后继节点的关系，并删除相应引用，让GC回收 Entry&lt;K,V&gt; e = (Entry&lt;K,V&gt;)tab[index]; for(Entry&lt;K,V&gt; prev = null ; e != null ; prev = e, e = e.next) &#123; if ((e.hash == hash) &amp;&amp; e.key.equals(key)) &#123; modCount++; if (prev != null) &#123; prev.next = e.next; &#125; else &#123; tab[index] = e.next; &#125; count--; V oldValue = e.value; e.value = null; return oldValue; &#125; &#125; return null;&#125; synchronized修饰，线程安全。 剩下方法有兴趣自己阅读源码，public方法都用synchronized修饰，确保线程安全，并发环境下，多线程竞争对象锁，效率低，不推荐使用。线程安全的Map推荐使用ConcurrentHashMap。 和HashMap对比 线程是否安全：HashMap是线程不安全的，HashTable是线程安全的；HashTable内部的方法基本都经过 synchronized修饰； 对Null key 和Null value的支持：HashMap中，null可以作为键，这样的键只有一个，可以有一个或多个键所对应的值为null；HashTable中key和value都不能为null，否则抛出空指针异常； 初始容量大小和每次扩充容量大小的不同： 3.1. 创建时如果不指定容量初始值，Hashtable默认的初始大小为11，之后每次扩容，容量变为原来的2n+1。HashMap默认的初始化大小为16。之后每次扩充，容量变为原来的2倍； 3.2. 创建时如果给定了容量初始值，那么Hashtable会直接使用你给定的大小，而HashMap会将其扩充 为2的幂次方大小。 底层数据结构：JDK1.8及以后的HashMap在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）时，将链表转化为红黑树，以减少搜索时间，Hashtable没有这样的机制。","categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"}]},{"title":"分布式问题分析","slug":"分布式问题分析","date":"2021-05-09T08:59:03.000Z","updated":"2023-06-04T13:14:15.191Z","comments":true,"path":"2021/05/09/分布式问题分析/","link":"","permalink":"http://example.com/2021/05/09/%E5%88%86%E5%B8%83%E5%BC%8F%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/","excerpt":"","text":"一、谈谈业务中使用分布式的场景分布式主要是为了提供可扩展性以及高可用性，业务中使用分布式的场景主要有分布式存储以及分布式计算。 分布式存储中可以将数据分片到多个节点上，不仅可以提高性能（可扩展性），同时也可以使用多个节点对同一份数据进行备份（高可用性）。 至于分布式计算，就是将一个大的计算任务分解成小任务分配到多个节点上去执行，再汇总每个小任务的执行结果得到最终结果。MapReduce 是分布式计算最好的例子。 二、分布式事务指事务的操作位于不同的节点上，需要保证事务的 AICD 特性。 产生原因 数据库分库分表； SOA 架构，比如一个电商网站将订单业务和库存业务分离出来放到不同的节点上。 应用场景 下单：减少库存、更新订单状态。库存和订单如果不在同一个数据库，就涉及分布式事务。 支付：买家账户扣款、卖家账户入账。买家和卖家账户信息如果不在同一个数据库，就涉及分布式事务。 解决方案1. 两阶段提交协议 两阶段提交 两阶段提交协议可以很好地解决分布式事务问题。它可以使用 XA 来实现，XA 包含两个部分：事务管理器和本地资源管理器。其中本地资源管理器往往由数据库实现，比如 Oracle、DB2 这些商业数据库都实现了 XA 接口；而事务管理器作为全局的协调者，负责各个本地资源的提交和回滚。 2. 消息中间件消息中间件也可称作消息系统 (MQ)，它本质上是一个暂存转发消息的一个中间件。在分布式应用当中，我们可以把一个业务操作转换成一个消息，比如支付宝的余额转入余额宝操作，支付宝系统执行减少余额操作之后向消息系统发送一个消息，余额宝系统订阅这条消息然后进行增加余额宝操作。 2.1 消息处理模型（一）点对点 （二）发布/订阅 2.2 消息的可靠性（一）发送端的可靠性 发送端完成操作后一定能将消息成功发送到消息系统。 实现方法：在本地数据建一张消息表，将消息数据与业务数据保存在同一数据库实例里，这样就可以利用本地数据库的事务机制。事务提交成功后，将消息表中的消息转移到消息中间件，若转移消息成功则删除消息表中的数据，否则继续重传。 （二）接收端的可靠性 接收端仅且能够从消息中间件成功消费一次消息。 实现方法： 保证接收端处理消息的业务逻辑具有幂等性：只要具有幂等性，那么消费多少次消息，最后处理的结果都是一样的。 保证消息具有唯一编号，并使用一张日志表来记录已经消费的消息编号。 三、负载均衡的算法与实现算法1. 轮询（Round Robin）轮询算法把每个请求轮流发送到每个服务器上。下图中，一共有 6 个客户端产生了 6 个请求，这 6 个请求按 (1, 2, 3, 4, 5, 6) 的顺序发送。最后，(1, 3, 5) 的请求会被发送到服务器 1，(2, 4, 6) 的请求会被发送到服务器 2。 该算法比较适合每个服务器的性能差不多的场景，如果有性能存在差异的情况下，那么性能较差的服务器可能无法承担多大的负载（下图的 Server 2）。 2. 加权轮询（Weighted Round Robbin）加权轮询是在轮询的基础上，根据服务器的性能差异，为服务器赋予一定的权值。例如下图中，服务器 1 被赋予的权值为 5，服务器 2 被赋予的权值为 1，那么 (1, 2, 3, 4, 5) 请求会被发送到服务器 1，(6) 请求会被发送到服务器 2。 3. 最少连接（least Connections）由于每个请求的连接时间不一样，使用轮询或者加权轮询算法的话，可能会让一台服务器当前连接数多大，而另一台服务器的连接多小，造成负载不均衡。例如下图中，(1, 3, 5) 请求会被发送到服务器 1，但是 (1, 3) 很快就断开连接，此时只有 (5) 请求连接服务器 1；(2, 4, 6) 请求被发送到服务器 2，只有 (2) 的连接断开。该系统继续运行时，服务器 2 会承担多大的负载。 最少连接算法就是将请求发送给当前最少连接数的服务器上。例如下图中，服务器 1 当前连接数最小，那么新到来的请求 6 就会被发送到服务器 1 上。 4. 加权最小连接（Weighted Least Connection）在最小连接的基础上，根据服务器的性能为每台服务器分配权重，根据权重计算出每台服务器能处理的连接数。 5. 随机算法（Random）把请求随机发送到服务器上。和轮询算法类似，该算法比较适合服务器性能差不多的场景。 实现1. HTTP 重定向HTTP 重定向负载均衡服务器收到 HTTP 请求之后会返回服务器的地址，并将该地址写入 HTTP 重定向响应中返回给浏览器，浏览器收到后需要再次发送请求。 缺点： 用户访问的延迟会增加； 如果负载均衡器宕机，就无法访问该站点。 2. DNS 重定向使用 DNS 作为负载均衡器，根据负载情况返回不同服务器的 IP 地址。大型网站基本使用了这种方式做为第一级负载均衡手段，然后在内部使用其它方式做第二级负载均衡。 缺点： DNS 查找表可能会被客户端缓存起来，那么之后的所有请求都会被重定向到同一个服务器。 3. 修改 MAC 地址使用 LVS（Linux Virtual Server）这种链路层负载均衡器，根据负载情况修改请求的 MAC 地址。 4. 修改 IP 地址在网络层修改请求的目的 IP 地址。 5. 代理自动配置正向代理与反向代理的区别： 正向代理：发生在客户端，是由用户主动发起的。比如翻墙，客户端通过主动访问代理服务器，让代理服务器获得需要的外网数据，然后转发回客户端。 反向代理：发生在服务器端，用户不知道代理的存在。 PAC 服务器是用来判断一个请求是否要经过代理。 四、分布式锁Java 提供了两种内置的锁的实现，一种是由 JVM 实现的 synchronized 和 JDK 提供的 Lock，对于单机单进程应用，可以使用它们来实现锁。当应用涉及到多机、多进程共同完成时，那么这时候就需要一个全局锁来实现多个进程之间的同步。 使用场景在服务器端使用分布式部署的情况下，一个服务可能分布在不同的节点上，比如订单服务分布式在节点 A 和节点 B 上。如果多个客户端同时对一个服务进行请求时，就需要使用分布式锁。例如一个服务可以使用 APP 端或者 Web 端进行访问，如果一个用户同时使用 APP 端和 Web 端访问该服务，并且 APP 端的请求路由到了节点 A，WEB 端的请求被路由到了节点 B，这时候就需要使用分布式锁来进行同步。 实现方式1. 数据库分布式锁（一）基于 MySQL 锁表 该实现完全依靠数据库的唯一索引。当想要获得锁时，就向数据库中插入一条记录，释放锁时就删除这条记录。如果记录具有唯一索引，就不会同时插入同一条记录。 这种方式存在以下几个问题： 锁没有失效时间，解锁失败会导致死锁，其他线程无法再获得锁。 只能是非阻塞锁，插入失败直接就报错了，无法重试。 不可重入，同一线程在没有释放锁之前无法再获得锁。 （二）采用乐观锁增加版本号 根据版本号来判断更新之前有没有其他线程更新过，如果被更新过，则获取锁失败。 2. Redis 分布式锁（一）基于 SETNX、EXPIRE 使用 SETNX（set if not exist）命令插入一个键值对时，如果 Key 已经存在，那么会返回 False，否则插入成功并返回 True。因此客户端在尝试获得锁时，先使用 SETNX 向 Redis 中插入一个记录，如果返回 True 表示获得锁，返回 False 表示已经有客户端占用锁。 EXPIRE 可以为一个键值对设置一个过期时间，从而避免了死锁的发生。 （二）RedLock 算法 RedLock 算法使用了多个 Redis 实例来实现分布式锁，这是为了保证在发生单点故障时还可用。 尝试从 N 个相互独立 Redis 实例获取锁，如果一个实例不可用，应该尽快尝试下一个。 计算获取锁消耗的时间，只有当这个时间小于锁的过期时间，并且从大多数（N/2+1）实例上获取了锁，那么就认为锁获取成功了。 如果锁获取失败，会到每个实例上释放锁。 3. Zookeeper 分布式锁Zookeeper 是一个为分布式应用提供一致性服务的软件，例如配置管理、分布式协同以及命名的中心化等，这些都是分布式系统中非常底层而且是必不可少的基本功能，但是如果自己实现这些功能而且要达到高吞吐、低延迟同时还要保持一致性和可用性，实际上非常困难。 （一）抽象模型 Zookeeper 提供了一种树形结构级的命名空间，/app1/p_1 节点表示它的父节点为 /app1。 （二）节点类型 永久节点：不会因为会话结束或者超时而消失； 临时节点：如果会话结束或者超时就会消失； 有序节点：会在节点名的后面加一个数字后缀，并且是有序的，例如生成的有序节点为 /lock/node-0000000000，它的下一个有序节点则为 /lock/node-0000000001，依次类推。 （三）监听器 为一个节点注册监听器，在节点状态发生改变时，会给客户端发送消息。 （四）分布式锁实现 创建一个锁目录 /lock。 在 /lock 下创建临时的且有序的子节点，第一个客户端对应的子节点为 /lock/lock-0000000000，第二个为 /lock/lock-0000000001，以此类推。 客户端获取 /lock 下的子节点列表，判断自己创建的子节点是否为当前子节点列表中序号最小的子节点，如果是则认为获得锁，否则监听自己的前一个子节点，获得子节点的变更通知后重复此步骤直至获得锁； 执行业务代码，完成后，删除对应的子节点。 （五）会话超时 如果一个已经获得锁的会话超时了，因为创建的是临时节点，因此该会话对应的临时节点会被删除，其它会话就可以获得锁了。可以看到，Zookeeper 分布式锁不会出现数据库分布式锁的死锁问题。 （六）羊群效应 在步骤二，一个节点未获得锁，需要监听监听自己的前一个子节点，这是因为如果监听所有的子节点，那么任意一个子节点状态改变，其它所有子节点都会收到通知（羊群效应），而我们只希望它的后一个子节点收到通知。 五、分布式 Session在分布式场景下，一个用户的 Session 如果只存储在一个服务器上，那么当负载均衡器把用户的下一个请求转发到另一个服务器上，该服务器没有用户的 Session，就可能导致用户需要重新进行登录等操作。 1. Sticky Sessions需要配置负载均衡器，使得一个用户的所有请求都路由到一个服务器节点上，这样就可以把用户的 Session 存放在该服务器节点中。 缺点：当服务器节点宕机时，将丢失该服务器节点上的所有 Session。 2. Session Replication在服务器节点之间进行 Session 同步操作，这样的话用户可以访问任何一个服务器节点。 缺点：需要更好的服务器硬件条件；需要对服务器进行配置。 3. Persistent DataStore将 Session 信息持久化到一个数据库中。 缺点：有可能需要去实现存取 Session 的代码。 4. In-Memory DataStore可以使用 Redis 和 Memcached 这种内存型数据库对 Session 进行存储，可以大大提高 Session 的读写效率。内存型数据库同样可以持久化数据到磁盘中来保证数据的安全性。 六、分库与分表带来的分布式困境与应对之策 事务问题使用分布式事务。 查询问题使用汇总表。 ID 唯一性 使用全局唯一 ID：GUID。 为每个分片指定一个 ID 范围。 分布式 ID 生成器 (如 Twitter 的 Snowflake 算法)。","categories":[],"tags":[{"name":"分布式","slug":"分布式","permalink":"http://example.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}]}],"categories":[],"tags":[{"name":"gis","slug":"gis","permalink":"http://example.com/tags/gis/"},{"name":"postgis","slug":"postgis","permalink":"http://example.com/tags/postgis/"},{"name":"sql","slug":"sql","permalink":"http://example.com/tags/sql/"},{"name":"spring","slug":"spring","permalink":"http://example.com/tags/spring/"},{"name":"schedule","slug":"schedule","permalink":"http://example.com/tags/schedule/"},{"name":"kubernets","slug":"kubernets","permalink":"http://example.com/tags/kubernets/"},{"name":"Java","slug":"Java","permalink":"http://example.com/tags/Java/"},{"name":"Postgrep","slug":"Postgrep","permalink":"http://example.com/tags/Postgrep/"},{"name":"分布式","slug":"分布式","permalink":"http://example.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"oracle","slug":"oracle","permalink":"http://example.com/tags/oracle/"},{"name":"spatial","slug":"spatial","permalink":"http://example.com/tags/spatial/"}]}